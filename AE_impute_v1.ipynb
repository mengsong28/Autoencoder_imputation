{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>An Autoencoder-Based Deep Learning Method \n",
    "for Genotype Imputation \n",
    " <span class=\"tocSkip\"></span></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This is the python code for AE model for genotype imputation on human HLA data from 1000 Genomes Project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Reshape, Flatten, Lambda, Input, Activation, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, UpSampling1D, Dropout, Dense, Reshape\n",
    "#from tensorflow.keras.regularizers import l1\n",
    "from tensorflow.keras.regularizers import L1\n",
    "\n",
    "from tensorflow.keras.layers import Conv1DTranspose\n",
    "\n",
    "%matplotlib inline   \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import scipy.stats as stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestr 2022/09/27-18:28:02\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "#timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "timestr = time.strftime(\"%Y/%m/%d-%H:%M:%S\")\n",
    "print('timestr', timestr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_path: /ihome/msong/Multi_omics_imputation_08302021/data\n"
     ]
    }
   ],
   "source": [
    "# set data path for user\n",
    "\n",
    "import os\n",
    "\n",
    "data_path = '/ihome/msong/Multi_omics_imputation_08302021/data'\n",
    "print('data_path:', data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load HLA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snp cols_label: ['HG00096', 'HG00097', 'HG00099', 'HG00100', 'HG00101', 'HG00102', 'HG00103', 'HG00105', 'HG00106', 'HG00107']\n",
      "snp rows_label:         POS\n",
      "0  28690555\n",
      "1  28690607\n",
      "2  28690649\n",
      "3  28690670\n",
      "4  28690688\n",
      "5  28690891\n",
      "6  28690948\n",
      "7  28691031\n",
      "snp shape: (27208, 2504)\n",
      "snp : [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 2 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "snp_new new : [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 3 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 2 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n",
      "snp_new_t shape: (2504, 27208)\n"
     ]
    }
   ],
   "source": [
    "# 07/23/2022: HLA new metrics (CR, PCC, IQS, HEN, SEN)\n",
    "# load HLA data\n",
    "import csv\n",
    "\n",
    "\n",
    "file_name = 'HLA_filterd_012_v2.csv'\n",
    "input_name = os.path.join(data_path, file_name)\n",
    "\n",
    "#print(input_name)\n",
    "\n",
    "\n",
    "feature_size = 27208\n",
    "sample_num = 2504 + 1\n",
    "\n",
    "\n",
    "# # small test case\n",
    "# feature_size = 128\n",
    "# sample_num = 200 + 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# header/columns labels\n",
    "f = open(input_name)\n",
    "reader = csv.reader(f)\n",
    "cols_label = next(reader)\n",
    "#cols_label = cols_label[9:sample_num]\n",
    "cols_label = cols_label[1:sample_num]\n",
    "print('snp cols_label:', cols_label[:10])\n",
    "\n",
    "lines2skip = 0\n",
    "rows_label = pd.read_csv(input_name, skiprows=(lines2skip+0), usecols=range(0,1))\n",
    "print('snp rows_label:', rows_label[:8])\n",
    "\n",
    "snp_cols_label = cols_label\n",
    "snp_rows_label = rows_label[0:feature_size]\n",
    "\n",
    "\n",
    "#snp = pd.read_csv(input_name, skiprows=lines2skip, usecols = range(9,sample_num), nrows=feature_size)\n",
    "snp = pd.read_csv(input_name, skiprows=lines2skip, usecols = range(1,sample_num), nrows=feature_size)\n",
    "\n",
    "print(\"snp shape:\", snp.shape)\n",
    "print(\"snp :\", snp.values[0:8, 0:20])\n",
    "\n",
    "# add 1 to all genotype: SCDA\n",
    "snp_new = snp + 1\n",
    "print(\"snp_new new :\", snp_new.values[0:8, 0:20])\n",
    "\n",
    "# transpose\n",
    "snp_new_t = snp_new.transpose()\n",
    "print(\"snp_new_t shape:\", snp_new_t.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_seed: [86509]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "run_times = 1\n",
    "#run_times = 3\n",
    "\n",
    "#print(random.randrange(20, 50, 3))\n",
    "random_seed = [0 for i in range(run_times)]\n",
    "\n",
    "for i in range(run_times):\n",
    "    random_seed[i] = random.randrange(28213, 28213*4, 12)\n",
    "\n",
    "    \n",
    "print('random_seed:', random_seed)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def beagle_r2(data_imp):\n",
    "\n",
    "    # 5.30.2022, Calculate Beagle R2 (without ground truth)\n",
    "    print('Beagle R2.')\n",
    "    \n",
    "    # predict\n",
    "    # SCDA\n",
    "    #predict_onehot = SCDA.predict(test_X_missing[:, :, :])\n",
    "    predict_onehot = data_imp\n",
    "    \n",
    "    print('predict_onehot:', predict_onehot.shape)\n",
    "    #print('predict_onehot:', predict_onehot[0:1, :, :])\n",
    "    \n",
    "    #predict_onehot_t = predict_onehot.transpose()\n",
    "    predict_onehot_t = predict_onehot.transpose(1, 0, 2)\n",
    "    print('predict_onehot_t:', predict_onehot_t.shape) \n",
    "#     print('predict_onehot_t:', predict_onehot_t[0:1, 0:5, :])\n",
    "    \n",
    "    snp_cnt = predict_onehot_t.shape[0]\n",
    "    sample_cnt = predict_onehot_t.shape[1]\n",
    "    print('snp_cnt:', snp_cnt) \n",
    "    print('sample_cnt:', sample_cnt) \n",
    "    \n",
    "    \n",
    "    # ui = yi(1) + 2yi(2)\n",
    "    u = [[0 for x in range(sample_cnt)] for y in range(snp_cnt)] \n",
    "    \n",
    "    # wi = yi(1) + 4yi(2)\n",
    "    w = [[0 for x in range(sample_cnt)] for y in range(snp_cnt)]\n",
    "    \n",
    "    # z: highest prob\n",
    "    z = [[0 for x in range(sample_cnt)] for y in range(snp_cnt)]\n",
    "    \n",
    "    R2 = [0 for x in range(snp_cnt)]\n",
    "    \n",
    "    for i in range(snp_cnt):\n",
    "        for j in range(sample_cnt):\n",
    "            u[i][j] = predict_onehot_t[i][j][2] + 2*predict_onehot_t[i][j][3]\n",
    "            \n",
    "            w[i][j] = predict_onehot_t[i][j][2] + 4*predict_onehot_t[i][j][3]\n",
    "            \n",
    "            #z[i][j] = predict_onehot_t[i][j].max()\n",
    "            z[i][j] = np.argmax(predict_onehot_t[i][j]) - 1\n",
    "            #print('u, w, z:', i, j, u[i][j], w[i][j], z[i][j])\n",
    "\n",
    "    \n",
    "    \n",
    "    # check constant values in z\n",
    "    mask_constant = [0 for x in range(snp_cnt)]\n",
    "    \n",
    "    for i in range(snp_cnt):\n",
    "        # result = len(listOfStrings) > 0 and all(elem == listOfStrings[0] for elem in listOfStrings)\n",
    "        \n",
    "        z_value = z[i]\n",
    "        #mask_constant[i] = len(listOfStrings) > 0 and all(elem == listOfStrings[0] for elem in listOfStrings)\n",
    "        flag = len(z_value) > 0 and all(elem == z_value[0] for elem in z_value)\n",
    "        if (flag == True):\n",
    "            mask_constant[i] = 1\n",
    "        \n",
    "    #print('mask_constant:', mask_constant)\n",
    "    \n",
    "    mask_constant_cnt = sum(mask_constant)\n",
    "    print('mask_constant_cnt:', mask_constant_cnt)\n",
    "    \n",
    "    for i in range(snp_cnt):\n",
    "        s_zu = 0\n",
    "        s_z = 0\n",
    "        s_u = 0\n",
    "        s_w = 0\n",
    "        s_zz = 0\n",
    "        \n",
    "        if(mask_constant[i] == 1):\n",
    "            R2[i] = 1\n",
    "#             print('Constant R2[i]:', i, R2[i])\n",
    "            \n",
    "        else:\n",
    "\n",
    "            for j in range(sample_cnt):\n",
    "                s_zu = s_zu + z[i][j]*u[i][j]\n",
    "                s_z = s_z + z[i][j]\n",
    "                s_u = s_u + u[i][j]\n",
    "\n",
    "                s_w = s_w + w[i][j]\n",
    "                s_zz = s_zz + z[i][j]*z[i][j]\n",
    "\n",
    "#             print('sum:', i, s_zu, s_z, s_u, s_w, s_zz)\n",
    "\n",
    "            #R2[i] = ((s_zu-(1/sample_cnt)*s_u*s_z)**2)/((s_w-(1/sample_cnt)*(s_u**2))*(s_zz-(1/sample_cnt)*(s_z**2)))  \n",
    "            tmp1 = (s_zu-(1/sample_cnt)*s_u*s_z)**2\n",
    "            tmp2 = s_w-(1/sample_cnt)*(s_u**2)\n",
    "            tmp3 = s_zz-(1/sample_cnt)*(s_z**2)\n",
    "#             print('R2 tmp1_2_3:', i, tmp1, tmp2, tmp3)\n",
    "\n",
    "            R2[i] = tmp1/(tmp2*tmp3)\n",
    "\n",
    "#             print('R2[i]:', i, R2[i])\n",
    "    \n",
    "#     print('R2:', R2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # check R2>=0.8\n",
    "    r2_th = 0.8\n",
    "    mask_valid = [0 for x in range(snp_cnt)]\n",
    "    \n",
    "    for i in range(snp_cnt):\n",
    "        if (R2[i] >= r2_th):\n",
    "            mask_valid[i] = 1\n",
    "            \n",
    "    mask_valid_cnt = sum(mask_valid)\n",
    "    print('R2 mask_valid_cnt:', mask_valid_cnt)\n",
    "    \n",
    "    mask_valid_perc = mask_valid_cnt/snp_cnt\n",
    "    print('R2 mask_valid_perc:', mask_valid_perc)\n",
    "\n",
    "    return R2, mask_valid_perc\n",
    "\n",
    "\n",
    "# 09/19/2022, pcc for dosage, from reviewer 1.\n",
    "\n",
    "def pcc(data_imp, data_obs):\n",
    "    # SCDA   \n",
    "    sample_cnt = data_imp.shape[0]\n",
    "    snp_cnt = data_imp.shape[1]\n",
    "    \n",
    "    \n",
    "    \n",
    "    dose_obs = [[0 for x in range(snp_cnt)] for y in range(sample_cnt)] \n",
    "    dose_imp = [[0 for x in range(snp_cnt)] for y in range(sample_cnt)]\n",
    " \n",
    "    for i in range(sample_cnt):\n",
    "        for j in range(snp_cnt):            \n",
    "            dose_obs[i][j] = 1*data_obs[i,j,2] + 2*data_obs[i,j,3]\n",
    "            dose_imp[i][j] = 1*data_imp[i,j,2] + 2*data_imp[i,j,3]\n",
    "\n",
    "    y_true = dose_obs\n",
    "    y_pred = dose_imp\n",
    "     \n",
    "    \n",
    "    \n",
    "    # pcc\n",
    "    rows = sample_cnt\n",
    "    cols = snp_cnt\n",
    "    \n",
    "    \n",
    "    print('rows:', rows)\n",
    "    print('cols:', cols)\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    r = []\n",
    "    p = []\n",
    "    rp = []\n",
    "\n",
    "    pcc_na_cnt = 0\n",
    "    \n",
    "    for i in range(cols):\n",
    "        a = y_true[:, i]\n",
    "        b = y_pred[:, i]\n",
    "\n",
    "#         print('a:', i, a)\n",
    "#         print('b:', i, b)\n",
    "\n",
    "        # check constant vector\n",
    "        flag1 = len(a) > 0 and all(elem == a[0] for elem in a)\n",
    "        flag2 = len(b) > 0 and all(elem == b[0] for elem in b)\n",
    "        \n",
    "        if (flag1 == True) or (flag2 == True):\n",
    "            r.append(np.nan)\n",
    "            p.append(np.nan)\n",
    "            rp.append((np.nan, np.nan))    \n",
    "    \n",
    "            pcc_na_cnt = pcc_na_cnt + 1\n",
    "            \n",
    "        else:\n",
    "            pcc = stats.pearsonr(a, b)\n",
    "            r.append(pcc[0])\n",
    "            p.append(pcc[1])\n",
    "            rp.append(pcc)\n",
    "\n",
    "    print('pcc_na_cnt:', pcc_na_cnt)\n",
    "    print('pcc_non-na_cnt:', (snp_cnt-pcc_na_cnt))\n",
    "\n",
    "\n",
    "    cols = len(rp)\n",
    "    #print('pcc non-na len:', cols)\n",
    "\n",
    "    rp_valid = []\n",
    "    for i in range(cols):\n",
    "        rp_r = rp[i][0]\n",
    "        rp_p = rp[i][1]\n",
    "\n",
    "        #if(rp_r>0.1) and (rp_p<0.05):\n",
    "        if(rp_r>0.8) and (rp_p<0.05):\n",
    "            \n",
    "            #rp_valid.append(rp[i])\n",
    "            rp_valid.append([i, rp[i]])\n",
    "\n",
    "    #print('pcc rp_valid:', rp_valid)\n",
    "    print('pcc rp_valid:', len(rp_valid))\n",
    "    \n",
    "    #pcc_valid_ratio = len(rp_valid)/cols\n",
    "    pcc_valid_ratio = len(rp_valid)/(snp_cnt-pcc_na_cnt)\n",
    "    \n",
    "   \n",
    "    print('pcc rp_valid ratio:', pcc_valid_ratio)   \n",
    "    \n",
    "    \n",
    "    return r, pcc_valid_ratio\n",
    "\n",
    "\n",
    "\n",
    "def hellinger_score(data_imp, data_obs):\n",
    "    #predict_onehot = SCDA.predict(test_X_missing[:, :, :])\n",
    "    predict_onehot = data_imp\n",
    "    \n",
    "    print('predict_onehot:', predict_onehot.shape)\n",
    "    #print('predict_onehot:', predict_onehot[0:1, :, :])\n",
    "    \n",
    "    # predict_onehot_t: rows (snps); cols (samples)\n",
    "    #predict_onehot_t = predict_onehot.transpose()\n",
    "    predict_onehot_t = predict_onehot.transpose(1, 0, 2)\n",
    "    print('predict_onehot_t:', predict_onehot_t.shape) \n",
    "#     print('predict_onehot_t:', predict_onehot_t[0:1, 0:5, :])\n",
    "    \n",
    "    snp_cnt = predict_onehot_t.shape[0]\n",
    "    sample_cnt = predict_onehot_t.shape[1]\n",
    "    print('snp_cnt:', snp_cnt) \n",
    "    print('sample_cnt:', sample_cnt) \n",
    "    \n",
    "    test_X_onehot = data_obs\n",
    "    test_X_onehot_t = test_X_onehot.transpose(1, 0, 2)\n",
    "    print('test_X_onehot_t:', test_X_onehot_t.shape)     \n",
    "\n",
    "    \n",
    "    hellinger_score = [[0 for x in range(sample_cnt)] for y in range(snp_cnt)] \n",
    " \n",
    "    for i in range(snp_cnt):\n",
    "        for j in range(sample_cnt):\n",
    "            \n",
    "            tmp = 0\n",
    "            f1 = test_X_onehot_t[i][j]\n",
    "            f2 = predict_onehot_t[i][j]\n",
    "            \n",
    "            for k in range(3):\n",
    "                tmp = tmp + np.sqrt(f1[k+1]*f2[k+1])\n",
    "\n",
    "            hellinger_score[i][j] = 1-np.sqrt(1-tmp)\n",
    "\n",
    "\n",
    "    hellinger_score_mean = [0 for x in range(snp_cnt)]\n",
    "    hellinger_score_std = [0 for x in range(snp_cnt)]\n",
    "    \n",
    "    hellinger_score_min = [0 for x in range(snp_cnt)]\n",
    "    hellinger_score_max = [0 for x in range(snp_cnt)]\n",
    "    \n",
    "    hellinger_score_median = [0 for x in range(snp_cnt)]\n",
    "    hellinger_score_quantile = [0 for x in range(snp_cnt)]\n",
    "    \n",
    "    for i in range(snp_cnt):\n",
    "        hellinger_score_mean[i] = np.mean(hellinger_score[i])\n",
    "        hellinger_score_std[i] = np.std(hellinger_score[i])\n",
    "\n",
    "        hellinger_score_min[i] = np.amin(hellinger_score[i])\n",
    "        hellinger_score_max[i] = np.amax(hellinger_score[i])\n",
    "\n",
    "        hellinger_score_median[i] = np.median(hellinger_score[i])\n",
    "        hellinger_score_quantile[i] = np.quantile(hellinger_score[i], 0.5)\n",
    "\n",
    "    \n",
    "    print('hellinger_score_mean mean:', np.mean(hellinger_score_mean))\n",
    "    print('hellinger_score_min mean:', np.mean(hellinger_score_min))\n",
    "    \n",
    "    \n",
    "    # check hellinger score > 0.45\n",
    "    hs_th = 0.45\n",
    "    \n",
    "    #hellinger_score_mean\n",
    "    mask_valid = [0 for x in range(snp_cnt)]\n",
    "    \n",
    "    for i in range(snp_cnt):\n",
    "        if (hellinger_score_mean[i] >= hs_th):\n",
    "            mask_valid[i] = 1\n",
    "            \n",
    "    mask_valid_cnt = sum(mask_valid)\n",
    "    print('hellinger_score_mean mask_valid_cnt:', mask_valid_cnt)\n",
    "    \n",
    "    mask_valid_perc_mean = mask_valid_cnt/snp_cnt\n",
    "    print('hellinger_score_mean mask_valid_perc:', mask_valid_perc_mean)\n",
    "    \n",
    "    #hellinger_score_min\n",
    "    mask_valid = [0 for x in range(snp_cnt)]\n",
    "    \n",
    "    for i in range(snp_cnt):\n",
    "        if (hellinger_score_min[i] >= hs_th):\n",
    "            mask_valid[i] = 1\n",
    "            \n",
    "    mask_valid_cnt = sum(mask_valid)\n",
    "    print('hellinger_score_min mask_valid_cnt:', mask_valid_cnt)\n",
    "    \n",
    "    mask_valid_perc_min = mask_valid_cnt/snp_cnt\n",
    "    print('hellinger_score_min mask_valid_perc:', mask_valid_perc_min)\n",
    "\n",
    "    return hellinger_score_mean, hellinger_score_min, mask_valid_perc_mean, mask_valid_perc_min\n",
    "\n",
    "\n",
    "\n",
    "def sen_score(data_imp, data_obs):\n",
    "    \n",
    "    #predict_onehot = SCDA.predict(test_X_missing[:, :, :])\n",
    "    predict_onehot = data_imp \n",
    "    \n",
    "    print('predict_onehot:', predict_onehot.shape)\n",
    "    #print('predict_onehot:', predict_onehot[0:1, :, :])\n",
    "    \n",
    "    # predict_onehot_t: rows (snps); cols (samples)\n",
    "    #predict_onehot_t = predict_onehot.transpose()\n",
    "    predict_onehot_t = predict_onehot.transpose(1, 0, 2)\n",
    "    print('predict_onehot_t:', predict_onehot_t.shape) \n",
    "#     print('predict_onehot_t:', predict_onehot_t[0:1, 0:5, :])\n",
    "    \n",
    "    snp_cnt = predict_onehot_t.shape[0]\n",
    "    sample_cnt = predict_onehot_t.shape[1]\n",
    "    print('snp_cnt:', snp_cnt) \n",
    "    print('sample_cnt:', sample_cnt) \n",
    "    \n",
    "    \n",
    "    test_X_onehot = data_obs\n",
    "    test_X_onehot_t = test_X_onehot.transpose(1, 0, 2)\n",
    "    print('test_X_onehot_t:', test_X_onehot_t.shape)     \n",
    "\n",
    "    \n",
    "    sen_score = [[0 for x in range(sample_cnt)] for y in range(snp_cnt)] \n",
    " \n",
    "    for i in range(snp_cnt):\n",
    "        for j in range(sample_cnt):\n",
    "            dose_obs = 1*test_X_onehot_t[i][j][2] + 2*test_X_onehot_t[i][j][3]\n",
    "            dose_imp = 1*predict_onehot_t[i][j][2] + 2*predict_onehot_t[i][j][3]\n",
    "            sen_score[i][j] = 1-((np.square(dose_obs-dose_imp))/4)\n",
    "\n",
    "\n",
    "    \n",
    "    sen_score_mean = [0 for x in range(snp_cnt)]\n",
    "    sen_score_std = [0 for x in range(snp_cnt)]\n",
    "    \n",
    "    sen_score_min = [0 for x in range(snp_cnt)]\n",
    "    sen_score_max = [0 for x in range(snp_cnt)]\n",
    "    \n",
    "    sen_score_median = [0 for x in range(snp_cnt)]\n",
    "    sen_score_quantile = [0 for x in range(snp_cnt)]\n",
    "    \n",
    "    for i in range(snp_cnt):\n",
    "        sen_score_mean[i] = np.mean(sen_score[i])\n",
    "        sen_score_std[i] = np.std(sen_score[i])\n",
    "\n",
    "        sen_score_min[i] = np.amin(sen_score[i])\n",
    "        sen_score_max[i] = np.amax(sen_score[i])\n",
    "\n",
    "        sen_score_median[i] = np.median(sen_score[i])\n",
    "        sen_score_quantile[i] = np.quantile(sen_score[i], 0.5)\n",
    "\n",
    "    \n",
    "    print('sen_score_mean mean:', np.mean(sen_score_mean))\n",
    "    print('sen_score_min mean:', np.mean(sen_score_min))\n",
    "\n",
    "    return sen_score_mean, sen_score_min, np.mean(sen_score_mean), np.mean(sen_score_min)\n",
    "    \n",
    "    \n",
    "def iqs_score(data_imp, data_obs):\n",
    "    #predict_onehot = SCDA.predict(test_X_missing[:, :, :])\n",
    "    predict_onehot = data_imp \n",
    "    \n",
    "    print('predict_onehot:', predict_onehot.shape)\n",
    "\n",
    "    # predict_onehot_t: rows (snps); cols (samples)\n",
    "    predict_onehot_t = predict_onehot.transpose(1, 0, 2)\n",
    "    \n",
    "    data_imp = np.argmax(predict_onehot_t, axis=2)\n",
    "   \n",
    "    data_imp_012 = data_imp - 1\n",
    "    \n",
    "    \n",
    "    print('data_imp_012:', data_imp_012.shape) \n",
    "    \n",
    "    snp_cnt = data_imp_012.shape[0]\n",
    "    sample_cnt = data_imp_012.shape[1]\n",
    "    print('snp_cnt:', snp_cnt) \n",
    "    print('sample_cnt:', sample_cnt) \n",
    "\n",
    "    test_X_onehot = data_obs\n",
    "    test_X_onehot_t = test_X_onehot.transpose(1, 0, 2)\n",
    "    print('test_X_onehot_t:', test_X_onehot_t.shape)     \n",
    "\n",
    "    data_obs = np.argmax(test_X_onehot_t, axis=2) \n",
    "    data_obs_012 = data_obs - 1    \n",
    "    \n",
    "    \n",
    "    iqs = [0 for x in range(snp_cnt)]    \n",
    "    \n",
    "    \n",
    "    for i in range(snp_cnt):\n",
    "        iqs_cnt = [[0 for x in range(3)] for y in range(3)]\n",
    "        \n",
    "        for j in range(sample_cnt):\n",
    "            if(data_obs_012[i][j]==0) and (data_imp_012[i][j]==0):\n",
    "                iqs_cnt[0][0] = iqs_cnt[0][0] + 1\n",
    "            elif(data_obs_012[i][j]==1) and (data_imp_012[i][j]==0):\n",
    "                iqs_cnt[0][1] = iqs_cnt[0][1] + 1\n",
    "            elif(data_obs_012[i][j]==2) and (data_imp_012[i][j]==0):\n",
    "                iqs_cnt[0][2] = iqs_cnt[0][2] + 1    \n",
    "\n",
    "            elif(data_obs_012[i][j]==0) and (data_imp_012[i][j]==1):\n",
    "                iqs_cnt[1][0] = iqs_cnt[1][0] + 1\n",
    "            elif(data_obs_012[i][j]==1) and (data_imp_012[i][j]==1):\n",
    "                iqs_cnt[1][1] = iqs_cnt[1][1] + 1    \n",
    "            elif(data_obs_012[i][j]==2) and (data_imp_012[i][j]==1):\n",
    "                iqs_cnt[1][2] = iqs_cnt[1][2] + 1\n",
    "                \n",
    "            elif(data_obs_012[i][j]==0) and (data_imp_012[i][j]==2):\n",
    "                iqs_cnt[2][0] = iqs_cnt[2][0] + 1\n",
    "            elif(data_obs_012[i][j]==1) and (data_imp_012[i][j]==2):\n",
    "                iqs_cnt[2][1] = iqs_cnt[2][1] + 1    \n",
    "            elif(data_obs_012[i][j]==2) and (data_imp_012[i][j]==2):\n",
    "                iqs_cnt[2][2] = iqs_cnt[2][2] + 1\n",
    "                \n",
    "        po = (iqs_cnt[0][0] + iqs_cnt[1][1]+ iqs_cnt[2][2])/sample_cnt\n",
    "        \n",
    "        pc_rows_0 = iqs_cnt[0][0] + iqs_cnt[0][1]+ iqs_cnt[0][2]\n",
    "        pc_rows_1 = iqs_cnt[1][0] + iqs_cnt[1][1]+ iqs_cnt[1][2]\n",
    "        pc_rows_2 = iqs_cnt[2][0] + iqs_cnt[2][1]+ iqs_cnt[2][2]        \n",
    "    \n",
    "        pc_cols_0 = iqs_cnt[0][0] + iqs_cnt[1][0]+ iqs_cnt[2][0]\n",
    "        pc_cols_1 = iqs_cnt[0][1] + iqs_cnt[1][1]+ iqs_cnt[2][1]\n",
    "        pc_cols_2 = iqs_cnt[0][2] + iqs_cnt[1][2]+ iqs_cnt[2][2]\n",
    "    \n",
    "        pc = (pc_rows_0*pc_cols_0 + pc_rows_1*pc_cols_1 + pc_rows_2*pc_cols_2)/(sample_cnt**2)\n",
    "    \n",
    "\n",
    "        if(pc == 1):\n",
    "            iqs[i] = 1\n",
    "        else:\n",
    "            iqs[i] = (po - pc)/(1 - pc)\n",
    "    \n",
    "    \n",
    "    #print('iqs:', iqs)\n",
    "    print('iqs mean:', np.mean(iqs))\n",
    "    return iqs, np.mean(iqs)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build AE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AE model parameters\n",
    "\n",
    "\n",
    "# missing ratio\n",
    "#missing_perc = 0.1\n",
    "#missing_perc = 0.0\n",
    "missing_perc = 0.2\n",
    "\n",
    "\n",
    "# epochs\n",
    "#epochs = 10   \n",
    "epochs = 60 \n",
    "#epochs = 100   # chr20 LOS 5K\n",
    "\n",
    "\n",
    "# training batch size\n",
    "#batch_size = 32   # u19, 4984 samples\n",
    "bs = 32\n",
    "\n",
    "\n",
    "# optimizer learning rate\n",
    "lr = 1e-3\n",
    "\n",
    "\n",
    "\n",
    "# l1 regulalization\n",
    "kr = 1e-4\n",
    "k_initial = 'glorot_uniform'\n",
    "\n",
    "\n",
    "kernel_len = 5\n",
    "\n",
    "#feature_size = feature_size\n",
    "\n",
    "# channel = inChannel\n",
    "channel = 4\n",
    "\n",
    "num_latent = 128\n",
    "p_size = 2\n",
    "ndf_num = 32\n",
    "\n",
    "\n",
    "#dr_rate = drop_prec\n",
    "dr_rate = 0.2  # avoid overfitting for missing ratio of 0.7\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AE model in one cell\n",
    "\n",
    "\n",
    "\n",
    "# # V1: verify our own ae model with yeast genotype data\n",
    "\n",
    "# # 2.2.8 build variatial autoencoder for snp with subclassing function. \n",
    "\n",
    "class SNP_ENCODER(Model):    \n",
    "    def __init__(self, feature_size, channel=channel, ndf=ndf_num, n_latent=num_latent, dr=dr_rate):    \n",
    "        super(SNP_ENCODER, self).__init__()\n",
    "        self.feature_size = feature_size\n",
    "        self.channel = channel\n",
    "        self.ndf = ndf\n",
    "        self.n_latent = n_latent\n",
    "        self.dr = dr # dropout rate\n",
    "\n",
    "        # object, can be saved in tf mode\n",
    "        self.stride=1\n",
    "        self.kl = 5\n",
    "\n",
    "        \n",
    "    def build(self, inputs): \n",
    "        #encoder\n",
    "        # dense layer 1\n",
    "        self.c1 = Conv1D(filters=self.ndf, kernel_size=self.kl, strides=self.stride, padding=\"same\", \n",
    "                      activation='relu', use_bias=True, \n",
    "                      kernel_initializer=k_initial, kernel_regularizer=L1(kr), \n",
    "                      input_shape=(self.feature_size, self.channel))\n",
    "        self.p1 = MaxPooling1D(pool_size=p_size)\n",
    "        self.drop1 = Dropout(rate=self.dr)\n",
    "        \n",
    "        # dense layer 2\n",
    "        self.c2 = Conv1D(filters=(2*self.ndf), kernel_size=self.kl, strides=self.stride, padding=\"same\", \n",
    "                      activation='relu', use_bias=True, kernel_initializer=k_initial, \n",
    "                      kernel_regularizer=L1(kr))\n",
    "        self.p2 = MaxPooling1D(pool_size=p_size)\n",
    "        self.drop2 = Dropout(rate=self.dr)\n",
    "        \n",
    "        # dense layer 3\n",
    "        self.c3 = Conv1D(filters=(4*self.ndf), kernel_size=self.kl, strides=1, padding=\"same\", \n",
    "                      activation='relu', use_bias=True, kernel_initializer=k_initial, \n",
    "                      kernel_regularizer=L1(kr))\n",
    "        self.p3 = MaxPooling1D(pool_size=p_size)\n",
    "        self.drop3 = Dropout(rate=self.dr)\n",
    "\n",
    "        super(SNP_ENCODER, self).build(inputs)\n",
    "    \n",
    "    def call(self, inputs, training=True):\n",
    "        #print('SNP_ENCODER training flag: ', training)\n",
    "        x = self.c1(inputs)\n",
    "        x = self.p1(x)\n",
    "        x = self.drop1(x, training=training)\n",
    "\n",
    "        x = self.c2(x)\n",
    "        x = self.p2(x)\n",
    "        x = self.drop2(x, training=training)\n",
    "        \n",
    "        x = self.c3(x)\n",
    "        return x\n",
    "\n",
    "    # AFAIK: The most convenient method to print model.summary() \n",
    "    # similar to the sequential or functional API like.\n",
    "    def build_graph(self):\n",
    "        x = Input(shape=(self.feature_size, self.channel))\n",
    "        return Model(inputs=[x], outputs=self.call(x))\n",
    "    \n",
    "    \n",
    "    \n",
    "# SNP_DECODER(keras.Model):   \n",
    "class SNP_DECODER(Model):     \n",
    "    def __init__(self, feature_size, channel=channel, ndf=ndf_num, n_latent=num_latent, dr=dr_rate):\n",
    "        super(SNP_DECODER, self).__init__()\n",
    "        self.feature_size = feature_size\n",
    "        self.channel = channel\n",
    "        self.ndf = ndf\n",
    "        self.n_latent = n_latent\n",
    "        self.dr = dr # dropout rate\n",
    "        \n",
    "        # object, can be saved in tf mode\n",
    "        self.stride=1\n",
    "        self.kl=5\n",
    "        \n",
    "    def build(self, inputs):\n",
    "        #decoder        \n",
    "        self.c1 = Conv1D(filters=(2*self.ndf), kernel_size=self.kl, strides=self.stride, padding=\"same\", \n",
    "                      activation='relu', use_bias=True, \n",
    "                      kernel_initializer=k_initial, kernel_regularizer=L1(kr),\n",
    "                      input_shape=((self.feature_size>>2), self.n_latent))\n",
    "        \n",
    "        \n",
    "        self.s1 = UpSampling1D(size=p_size)\n",
    "        self.drop1 = Dropout(rate=self.dr)\n",
    "        \n",
    "        # dense layer 2\n",
    "        self.c2 = Conv1D(filters=(1*self.ndf), kernel_size=self.kl, strides=self.stride, padding=\"same\", \n",
    "                      activation='relu', use_bias=True, \n",
    "                      kernel_initializer=k_initial, kernel_regularizer=L1(kr))\n",
    "\n",
    "        self.s2 = UpSampling1D(size=p_size)\n",
    "        self.drop2 = Dropout(rate=self.dr)\n",
    "        \n",
    "\n",
    "        # dense layer6\n",
    "        self.c3 = Conv1D(filters=self.channel, kernel_size=self.kl, strides=1, padding=\"same\", \n",
    "                      activation='softmax', use_bias=True)\n",
    "        \n",
    "        super(SNP_DECODER, self).build(inputs)\n",
    "        \n",
    "    def call(self, inputs, training = True):\n",
    "        #print('SNP_DECODER training flag: ', training)\n",
    "        x = self.c1(inputs)\n",
    "        x = self.s1(x)\n",
    "        x = self.drop1(x, training=training)\n",
    "        \n",
    "        x = self.c2(x)\n",
    "        x = self.s2(x)\n",
    "        x = self.drop2(x, training=training)\n",
    "\n",
    "        \n",
    "        d_out = self.c3(x)\n",
    "        return d_out\n",
    "    \n",
    "    # AFAIK: The most convenient method to print model.summary() \n",
    "    # similar to the sequential or functional API like.\n",
    "    def build_graph(self):\n",
    "        x = Input(shape=(self.feature_size>>2, self.n_latent))\n",
    "        \n",
    "        return Model(inputs=[x], outputs=self.call(x))\n",
    "    \n",
    "\n",
    "    \n",
    "#class SNP_AE(keras.Model):\n",
    "class SNP_AE(Model):    \n",
    "    def __init__(self, feature_size, channel=channel, ndf=ndf_num, n_latent=num_latent, dr=dr_rate):    \n",
    "        super(SNP_AE, self).__init__()\n",
    "        self.feature_size = feature_size\n",
    "        self.channel = channel\n",
    "        self.ndf = ndf\n",
    "        self.n_latent = n_latent\n",
    "        self.dr = dr # dropout rate      \n",
    "        \n",
    "        self.encoder = SNP_ENCODER(self.feature_size)\n",
    "        self.decoder = SNP_DECODER(self.feature_size)\n",
    "    \n",
    "\n",
    "    def call(self, x, training=True): \n",
    "        latent = self.encoder(x, training)     \n",
    "        res = self.decoder(latent, training)\n",
    "\n",
    "        return res, latent\n",
    "    \n",
    "    # AFAIK: The most convenient method to print model.summary() \n",
    "    # similar to the sequential or functional API like.\n",
    "    def build_graph(self):\n",
    "        x = Input(shape=(self.feature_size, self.channel))\n",
    "        return Model(inputs=[x], outputs=self.call(x))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 27208, 4)]        0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 27208, 32)         672       \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 13604, 32)         0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 13604, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 13604, 64)         10304     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 6802, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 6802, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 6802, 128)         41088     \n",
      "=================================================================\n",
      "Total params: 52,064\n",
      "Trainable params: 52,064\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# plot snp vae encoder model\n",
    "\n",
    "SNP_encoder = SNP_ENCODER(feature_size)\n",
    "SNP_encoder.build((None, feature_size, channel))\n",
    "SNP_encoder.build_graph().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 6802, 128)]       0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 6802, 64)          41024     \n",
      "_________________________________________________________________\n",
      "up_sampling1d (UpSampling1D) (None, 13604, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 13604, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 13604, 32)         10272     \n",
      "_________________________________________________________________\n",
      "up_sampling1d_1 (UpSampling1 (None, 27208, 32)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 27208, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 27208, 4)          644       \n",
      "=================================================================\n",
      "Total params: 51,940\n",
      "Trainable params: 51,940\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# plot snp vae decoder model\n",
    "\n",
    "SNP_decoder = SNP_DECODER(feature_size)\n",
    "\n",
    "SNP_decoder.build((None, feature_size>>2, num_latent))\n",
    "SNP_decoder.build_graph().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 27208, 4)]        0         \n",
      "_________________________________________________________________\n",
      "snp_encoder_3 (SNP_ENCODER)  (None, 6802, 128)         52064     \n",
      "_________________________________________________________________\n",
      "snp_decoder_3 (SNP_DECODER)  (None, 27208, 4)          51940     \n",
      "=================================================================\n",
      "Total params: 104,004\n",
      "Trainable params: 104,004\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# plot snp vae model\n",
    "\n",
    "SNP_ae = SNP_AE(feature_size)\n",
    "SNP_ae.build((None, feature_size, channel))\n",
    "SNP_ae.build_graph().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AE model sub-functions\n",
    "\n",
    "def generate_fake_missing(x_in, missing_ratio):\n",
    "\n",
    "        # Generates missing genotypes\n",
    "        # different missing loci for each individuals\n",
    "        x_fake = x_in.copy()   # with .copy() to not overwrite the original data\n",
    "        \n",
    "        for i in range(x_in.shape[0]):\n",
    "            missing_size = int(missing_ratio * x_in.shape[1])\n",
    "          \n",
    "            # without repeat random numbers: set replace with false\n",
    "            missing_index = np.random.choice(x_in.shape[1], size=missing_size, replace=False)\n",
    "            \n",
    "            # missing loci are encoded as [0, 0]\n",
    "            #x_fake[i, missing_index, :] = [1, 0, 0]  # yeast\n",
    "            x_fake[i, missing_index, :] = [1, 0, 0, 0]  # human\n",
    "\n",
    "        return x_fake\n",
    "        #return x_fake, x_in\n",
    "    \n",
    "    \n",
    "\n",
    "def loss_function_cce(recon_x, x):\n",
    "    # orders: y_true, y_pred\n",
    "    cce = categorical_crossentropy(x, recon_x)\n",
    "\n",
    "    #cce = np.double(cce)\n",
    "    cce = K.cast(cce, dtype='float32')\n",
    "\n",
    "    lamb1 = 1.0\n",
    "    loss = lamb1*cce\n",
    "    #print('loss:', loss)\n",
    "    \n",
    "    loss = tf.reduce_mean(loss)\n",
    "    #print('ave loss:', loss)\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_size: 27208\n",
      "epochs: 60\n",
      "bs: 32\n",
      "missing_ratio: 0.2\n",
      "random_seed: [86509]\n"
     ]
    }
   ],
   "source": [
    "# check AE model parameters\n",
    "\n",
    "#epochs = 100   # chr20 LOS 5K\n",
    "\n",
    "print('feature_size:', feature_size)\n",
    "print('epochs:', epochs)\n",
    "\n",
    "#bs = 20\n",
    "#bs = batch_size\n",
    "#bs = 32\n",
    "print('bs:', bs)\n",
    "\n",
    "#missing_ratio = 0\n",
    "#missing_ratio = 0.1\n",
    "#missing_ratio = 0.2\n",
    "missing_ratio = missing_perc\n",
    "print('missing_ratio:', missing_ratio)\n",
    "\n",
    "print('random_seed:', random_seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train AE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#**********************************************************************#:\n",
      "start of k_th time: 0\n",
      "start of k_th seed: 0 86509\n",
      "df_ori shape: (2003, 27208)\n",
      "test_X shape: (501, 27208)\n",
      "df_onehot.shape: (2003, 27208, 4)\n",
      "train_X.shape: (1602, 27208, 4)\n",
      "valid_X.shape: (401, 27208, 4)\n",
      "train_X_fake diff: 17432964.0\n",
      "valid_X_fake diff: 4363682.0\n",
      "Start of epoch 0\n",
      "Training loss over epoch:  0 0.3443746\n",
      "Training acc over epoch:  0 0.9331229\n",
      "Validation loss:  0 0.2409186\n",
      "Validation acc:  0 0.9432055\n",
      "epoch 0: loss = 0.34437463\n",
      "Start of epoch 1\n",
      "Training loss over epoch:  1 0.22665806\n",
      "Training acc over epoch:  1 0.94235814\n",
      "Validation loss:  1 0.19393162\n",
      "Validation acc:  1 0.9432071\n",
      "epoch 1: loss = 0.22665802\n",
      "Start of epoch 2\n",
      "Training loss over epoch:  2 0.18122774\n",
      "Training acc over epoch:  2 0.9424873\n",
      "Validation loss:  2 0.15142624\n",
      "Validation acc:  2 0.95011884\n",
      "epoch 2: loss = 0.18122777\n",
      "Start of epoch 3\n",
      "Training loss over epoch:  3 0.15303712\n",
      "Training acc over epoch:  3 0.9461842\n",
      "Validation loss:  3 0.12959546\n",
      "Validation acc:  3 0.9644337\n",
      "epoch 3: loss = 0.15303712\n",
      "Start of epoch 4\n",
      "Training loss over epoch:  4 0.13284464\n",
      "Training acc over epoch:  4 0.95521116\n",
      "Validation loss:  4 0.107857384\n",
      "Validation acc:  4 0.9732217\n",
      "epoch 4: loss = 0.13284463\n",
      "Start of epoch 5\n",
      "Training loss over epoch:  5 0.11406578\n",
      "Training acc over epoch:  5 0.966293\n",
      "Validation loss:  5 0.08950941\n",
      "Validation acc:  5 0.9787728\n",
      "epoch 5: loss = 0.11406578\n",
      "Start of epoch 6\n",
      "Training loss over epoch:  6 0.10005669\n",
      "Training acc over epoch:  6 0.97348684\n",
      "Validation loss:  6 0.07825026\n",
      "Validation acc:  6 0.98266405\n",
      "epoch 6: loss = 0.100056686\n",
      "Start of epoch 7\n",
      "Training loss over epoch:  7 0.0920007\n",
      "Training acc over epoch:  7 0.9770506\n",
      "Validation loss:  7 0.07285019\n",
      "Validation acc:  7 0.9843543\n",
      "epoch 7: loss = 0.092000715\n",
      "Start of epoch 8\n",
      "Training loss over epoch:  8 0.087295234\n",
      "Training acc over epoch:  8 0.97884905\n",
      "Validation loss:  8 0.0699421\n",
      "Validation acc:  8 0.9852675\n",
      "epoch 8: loss = 0.08729523\n",
      "Start of epoch 9\n",
      "Training loss over epoch:  9 0.08405068\n",
      "Training acc over epoch:  9 0.9800555\n",
      "Validation loss:  9 0.06757756\n",
      "Validation acc:  9 0.9857744\n",
      "epoch 9: loss = 0.084050685\n",
      "Start of epoch 10\n",
      "Training loss over epoch:  10 0.081486836\n",
      "Training acc over epoch:  10 0.9809362\n",
      "Validation loss:  10 0.06541414\n",
      "Validation acc:  10 0.9862508\n",
      "epoch 10: loss = 0.08148685\n",
      "Start of epoch 11\n",
      "Training loss over epoch:  11 0.07941527\n",
      "Training acc over epoch:  11 0.9816518\n",
      "Validation loss:  11 0.063464195\n",
      "Validation acc:  11 0.9867111\n",
      "epoch 11: loss = 0.079415284\n",
      "Start of epoch 12\n",
      "Training loss over epoch:  12 0.07766345\n",
      "Training acc over epoch:  12 0.9822499\n",
      "Validation loss:  12 0.0624926\n",
      "Validation acc:  12 0.98695\n",
      "epoch 12: loss = 0.077663444\n",
      "Start of epoch 13\n",
      "Training loss over epoch:  13 0.07621907\n",
      "Training acc over epoch:  13 0.9827117\n",
      "Validation loss:  13 0.06135104\n",
      "Validation acc:  13 0.98722947\n",
      "epoch 13: loss = 0.07621907\n",
      "Start of epoch 14\n",
      "Training loss over epoch:  14 0.07497468\n",
      "Training acc over epoch:  14 0.98313\n",
      "Validation loss:  14 0.060195513\n",
      "Validation acc:  14 0.98737454\n",
      "epoch 14: loss = 0.074974686\n",
      "Start of epoch 15\n",
      "Training loss over epoch:  15 0.07390156\n",
      "Training acc over epoch:  15 0.9834592\n",
      "Validation loss:  15 0.059631478\n",
      "Validation acc:  15 0.9875017\n",
      "epoch 15: loss = 0.07390158\n",
      "Start of epoch 16\n",
      "Training loss over epoch:  16 0.072949424\n",
      "Training acc over epoch:  16 0.98373586\n",
      "Validation loss:  16 0.059105318\n",
      "Validation acc:  16 0.9875913\n",
      "epoch 16: loss = 0.07294941\n",
      "Start of epoch 17\n",
      "Training loss over epoch:  17 0.072042234\n",
      "Training acc over epoch:  17 0.9839953\n",
      "Validation loss:  17 0.058130976\n",
      "Validation acc:  17 0.9877246\n",
      "epoch 17: loss = 0.072042234\n",
      "Start of epoch 18\n",
      "Training loss over epoch:  18 0.07120291\n",
      "Training acc over epoch:  18 0.98422295\n",
      "Validation loss:  18 0.057585135\n",
      "Validation acc:  18 0.9877989\n",
      "epoch 18: loss = 0.0712029\n",
      "Start of epoch 19\n",
      "Training loss over epoch:  19 0.070506416\n",
      "Training acc over epoch:  19 0.9844281\n",
      "Validation loss:  19 0.0570156\n",
      "Validation acc:  19 0.9878659\n",
      "epoch 19: loss = 0.070506416\n",
      "Start of epoch 20\n",
      "Training loss over epoch:  20 0.06982427\n",
      "Training acc over epoch:  20 0.9845946\n",
      "Validation loss:  20 0.056476586\n",
      "Validation acc:  20 0.98793083\n",
      "epoch 20: loss = 0.069824256\n",
      "Start of epoch 21\n",
      "Training loss over epoch:  21 0.069210164\n",
      "Training acc over epoch:  21 0.9847555\n",
      "Validation loss:  21 0.0562615\n",
      "Validation acc:  21 0.9879927\n",
      "epoch 21: loss = 0.069210164\n",
      "Start of epoch 22\n",
      "Training loss over epoch:  22 0.06865811\n",
      "Training acc over epoch:  22 0.9848799\n",
      "Validation loss:  22 0.055473045\n",
      "Validation acc:  22 0.9880351\n",
      "epoch 22: loss = 0.06865812\n",
      "Start of epoch 23\n",
      "Training loss over epoch:  23 0.06802409\n",
      "Training acc over epoch:  23 0.9850398\n",
      "Validation loss:  23 0.055069335\n",
      "Validation acc:  23 0.9880871\n",
      "epoch 23: loss = 0.06802409\n",
      "Start of epoch 24\n",
      "Training loss over epoch:  24 0.06750088\n",
      "Training acc over epoch:  24 0.9851617\n",
      "Validation loss:  24 0.05468847\n",
      "Validation acc:  24 0.98813885\n",
      "epoch 24: loss = 0.06750089\n",
      "Start of epoch 25\n",
      "Training loss over epoch:  25 0.06702247\n",
      "Training acc over epoch:  25 0.98526645\n",
      "Validation loss:  25 0.05429188\n",
      "Validation acc:  25 0.988159\n",
      "epoch 25: loss = 0.067022465\n",
      "Start of epoch 26\n",
      "Training loss over epoch:  26 0.06650624\n",
      "Training acc over epoch:  26 0.98538977\n",
      "Validation loss:  26 0.05391604\n",
      "Validation acc:  26 0.98820496\n",
      "epoch 26: loss = 0.06650624\n",
      "Start of epoch 27\n",
      "Training loss over epoch:  27 0.0661043\n",
      "Training acc over epoch:  27 0.98545927\n",
      "Validation loss:  27 0.053559765\n",
      "Validation acc:  27 0.98824084\n",
      "epoch 27: loss = 0.06610429\n",
      "Start of epoch 28\n",
      "Training loss over epoch:  28 0.06570507\n",
      "Training acc over epoch:  28 0.9855491\n",
      "Validation loss:  28 0.053453397\n",
      "Validation acc:  28 0.98825157\n",
      "epoch 28: loss = 0.06570508\n",
      "Start of epoch 29\n",
      "Training loss over epoch:  29 0.065352865\n",
      "Training acc over epoch:  29 0.98564464\n",
      "Validation loss:  29 0.05294515\n",
      "Validation acc:  29 0.9882865\n",
      "epoch 29: loss = 0.065352865\n",
      "Start of epoch 30\n",
      "Training loss over epoch:  30 0.065008\n",
      "Training acc over epoch:  30 0.9856998\n",
      "Validation loss:  30 0.0528709\n",
      "Validation acc:  30 0.988307\n",
      "epoch 30: loss = 0.065008\n",
      "Start of epoch 31\n",
      "Training loss over epoch:  31 0.06465823\n",
      "Training acc over epoch:  31 0.98575246\n",
      "Validation loss:  31 0.05247387\n",
      "Validation acc:  31 0.9883252\n",
      "epoch 31: loss = 0.064658225\n",
      "Start of epoch 32\n",
      "Training loss over epoch:  32 0.06436099\n",
      "Training acc over epoch:  32 0.9858209\n",
      "Validation loss:  32 0.052404523\n",
      "Validation acc:  32 0.988331\n",
      "epoch 32: loss = 0.064361\n",
      "Start of epoch 33\n",
      "Training loss over epoch:  33 0.06415916\n",
      "Training acc over epoch:  33 0.9858633\n",
      "Validation loss:  33 0.052198768\n",
      "Validation acc:  33 0.9883452\n",
      "epoch 33: loss = 0.06415916\n",
      "Start of epoch 34\n",
      "Training loss over epoch:  34 0.06381425\n",
      "Training acc over epoch:  34 0.98593146\n",
      "Validation loss:  34 0.05191144\n",
      "Validation acc:  34 0.9883631\n",
      "epoch 34: loss = 0.06381424\n",
      "Start of epoch 35\n",
      "Training loss over epoch:  35 0.06364394\n",
      "Training acc over epoch:  35 0.9859588\n",
      "Validation loss:  35 0.051801212\n",
      "Validation acc:  35 0.98841345\n",
      "epoch 35: loss = 0.06364395\n",
      "Start of epoch 36\n",
      "Training loss over epoch:  36 0.06331464\n",
      "Training acc over epoch:  36 0.98602194\n",
      "Validation loss:  36 0.05159808\n",
      "Validation acc:  36 0.9883934\n",
      "epoch 36: loss = 0.063314654\n",
      "Start of epoch 37\n",
      "Training loss over epoch:  37 0.06309952\n",
      "Training acc over epoch:  37 0.98605686\n",
      "Validation loss:  37 0.051603\n",
      "Validation acc:  37 0.98840445\n",
      "epoch 37: loss = 0.063099526\n",
      "Start of epoch 38\n",
      "Training loss over epoch:  38 0.06285325\n",
      "Training acc over epoch:  38 0.98611945\n",
      "Validation loss:  38 0.051352557\n",
      "Validation acc:  38 0.9884344\n",
      "epoch 38: loss = 0.06285325\n",
      "Start of epoch 39\n",
      "Training loss over epoch:  39 0.06265878\n",
      "Training acc over epoch:  39 0.986128\n",
      "Validation loss:  39 0.051108196\n",
      "Validation acc:  39 0.9884612\n",
      "epoch 39: loss = 0.062658764\n",
      "Start of epoch 40\n",
      "Training loss over epoch:  40 0.0623829\n",
      "Training acc over epoch:  40 0.9861962\n",
      "Validation loss:  40 0.051051766\n",
      "Validation acc:  40 0.98845786\n",
      "epoch 40: loss = 0.062382907\n",
      "Start of epoch 41\n",
      "Training loss over epoch:  41 0.062251084\n",
      "Training acc over epoch:  41 0.9862123\n",
      "Validation loss:  41 0.0507939\n",
      "Validation acc:  41 0.9884759\n",
      "epoch 41: loss = 0.062251076\n",
      "Start of epoch 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss over epoch:  42 0.06209218\n",
      "Training acc over epoch:  42 0.98624045\n",
      "Validation loss:  42 0.050694216\n",
      "Validation acc:  42 0.98849595\n",
      "epoch 42: loss = 0.06209219\n",
      "Start of epoch 43\n",
      "Training loss over epoch:  43 0.06184948\n",
      "Training acc over epoch:  43 0.98629403\n",
      "Validation loss:  43 0.050592013\n",
      "Validation acc:  43 0.9885055\n",
      "epoch 43: loss = 0.061849475\n",
      "Start of epoch 44\n",
      "Training loss over epoch:  44 0.06164125\n",
      "Training acc over epoch:  44 0.98632073\n",
      "Validation loss:  44 0.050347082\n",
      "Validation acc:  44 0.98855233\n",
      "epoch 44: loss = 0.06164126\n",
      "Start of epoch 45\n",
      "Training loss over epoch:  45 0.061521284\n",
      "Training acc over epoch:  45 0.98634374\n",
      "Validation loss:  45 0.050153557\n",
      "Validation acc:  45 0.98857665\n",
      "epoch 45: loss = 0.061521288\n",
      "Start of epoch 46\n",
      "Training loss over epoch:  46 0.061321273\n",
      "Training acc over epoch:  46 0.9863855\n",
      "Validation loss:  46 0.050017543\n",
      "Validation acc:  46 0.9885837\n",
      "epoch 46: loss = 0.06132128\n",
      "Start of epoch 47\n",
      "Training loss over epoch:  47 0.061227288\n",
      "Training acc over epoch:  47 0.9863995\n",
      "Validation loss:  47 0.05014797\n",
      "Validation acc:  47 0.98856544\n",
      "epoch 47: loss = 0.06122729\n",
      "Start of epoch 48\n",
      "Training loss over epoch:  48 0.060936604\n",
      "Training acc over epoch:  48 0.98644185\n",
      "Validation loss:  48 0.049967747\n",
      "Validation acc:  48 0.9885977\n",
      "epoch 48: loss = 0.060936615\n",
      "Start of epoch 49\n",
      "Training loss over epoch:  49 0.060797773\n",
      "Training acc over epoch:  49 0.98649067\n",
      "Validation loss:  49 0.04981736\n",
      "Validation acc:  49 0.98863703\n",
      "epoch 49: loss = 0.060797773\n",
      "Start of epoch 50\n",
      "Training loss over epoch:  50 0.060623307\n",
      "Training acc over epoch:  50 0.98650444\n",
      "Validation loss:  50 0.049580254\n",
      "Validation acc:  50 0.9886516\n",
      "epoch 50: loss = 0.060623318\n",
      "Start of epoch 51\n",
      "Training loss over epoch:  51 0.06050942\n",
      "Training acc over epoch:  51 0.9865307\n",
      "Validation loss:  51 0.049521208\n",
      "Validation acc:  51 0.98865265\n",
      "epoch 51: loss = 0.06050942\n",
      "Start of epoch 52\n",
      "Training loss over epoch:  52 0.060362995\n",
      "Training acc over epoch:  52 0.9865581\n",
      "Validation loss:  52 0.049450144\n",
      "Validation acc:  52 0.9886514\n",
      "epoch 52: loss = 0.06036299\n",
      "Start of epoch 53\n",
      "Training loss over epoch:  53 0.060187325\n",
      "Training acc over epoch:  53 0.9865839\n",
      "Validation loss:  53 0.049351454\n",
      "Validation acc:  53 0.98865825\n",
      "epoch 53: loss = 0.060187336\n",
      "Start of epoch 54\n",
      "Training loss over epoch:  54 0.060018618\n",
      "Training acc over epoch:  54 0.9866117\n",
      "Validation loss:  54 0.049112543\n",
      "Validation acc:  54 0.988718\n",
      "epoch 54: loss = 0.060018614\n",
      "Start of epoch 55\n",
      "Training loss over epoch:  55 0.05990648\n",
      "Training acc over epoch:  55 0.98663914\n",
      "Validation loss:  55 0.049232233\n",
      "Validation acc:  55 0.98868114\n",
      "epoch 55: loss = 0.059906486\n",
      "Start of epoch 56\n",
      "Training loss over epoch:  56 0.059744865\n",
      "Training acc over epoch:  56 0.98666936\n",
      "Validation loss:  56 0.049114607\n",
      "Validation acc:  56 0.9887263\n",
      "epoch 56: loss = 0.05974487\n",
      "Start of epoch 57\n",
      "Training loss over epoch:  57 0.059634224\n",
      "Training acc over epoch:  57 0.986685\n",
      "Validation loss:  57 0.048853647\n",
      "Validation acc:  57 0.98875684\n",
      "epoch 57: loss = 0.059634227\n",
      "Start of epoch 58\n",
      "Training loss over epoch:  58 0.05946094\n",
      "Training acc over epoch:  58 0.98671836\n",
      "Validation loss:  58 0.048807837\n",
      "Validation acc:  58 0.988738\n",
      "epoch 58: loss = 0.059460934\n",
      "Start of epoch 59\n",
      "Training loss over epoch:  59 0.059408143\n",
      "Training acc over epoch:  59 0.98674726\n",
      "Validation loss:  59 0.048850156\n",
      "Validation acc:  59 0.9887309\n",
      "epoch 59: loss = 0.05940815\n",
      "loss_vae: [0.34437463, 0.22665802, 0.18122777, 0.15303712, 0.13284463, 0.11406578, 0.100056686, 0.092000715, 0.08729523, 0.084050685, 0.08148685, 0.079415284, 0.077663444, 0.07621907, 0.074974686, 0.07390158, 0.07294941, 0.072042234, 0.0712029, 0.070506416, 0.069824256, 0.069210164, 0.06865812, 0.06802409, 0.06750089, 0.067022465, 0.06650624, 0.06610429, 0.06570508, 0.065352865, 0.065008, 0.064658225, 0.064361, 0.06415916, 0.06381424, 0.06364395, 0.063314654, 0.063099526, 0.06285325, 0.062658764, 0.062382907, 0.062251076, 0.06209219, 0.061849475, 0.06164126, 0.061521288, 0.06132128, 0.06122729, 0.060936615, 0.060797773, 0.060623318, 0.06050942, 0.06036299, 0.060187336, 0.060018614, 0.059906486, 0.05974487, 0.059634227, 0.059460934, 0.05940815]\n",
      "loss train: [0.3443746, 0.22665806, 0.18122774, 0.15303712, 0.13284464, 0.11406578, 0.10005669, 0.0920007, 0.087295234, 0.08405068, 0.081486836, 0.07941527, 0.07766345, 0.07621907, 0.07497468, 0.07390156, 0.072949424, 0.072042234, 0.07120291, 0.070506416, 0.06982427, 0.069210164, 0.06865811, 0.06802409, 0.06750088, 0.06702247, 0.06650624, 0.0661043, 0.06570507, 0.065352865, 0.065008, 0.06465823, 0.06436099, 0.06415916, 0.06381425, 0.06364394, 0.06331464, 0.06309952, 0.06285325, 0.06265878, 0.0623829, 0.062251084, 0.06209218, 0.06184948, 0.06164125, 0.061521284, 0.061321273, 0.061227288, 0.060936604, 0.060797773, 0.060623307, 0.06050942, 0.060362995, 0.060187325, 0.060018618, 0.05990648, 0.059744865, 0.059634224, 0.05946094, 0.059408143]\n",
      "loss val: [0.2409186, 0.19393162, 0.15142624, 0.12959546, 0.107857384, 0.08950941, 0.07825026, 0.07285019, 0.0699421, 0.06757756, 0.06541414, 0.063464195, 0.0624926, 0.06135104, 0.060195513, 0.059631478, 0.059105318, 0.058130976, 0.057585135, 0.0570156, 0.056476586, 0.0562615, 0.055473045, 0.055069335, 0.05468847, 0.05429188, 0.05391604, 0.053559765, 0.053453397, 0.05294515, 0.0528709, 0.05247387, 0.052404523, 0.052198768, 0.05191144, 0.051801212, 0.05159808, 0.051603, 0.051352557, 0.051108196, 0.051051766, 0.0507939, 0.050694216, 0.050592013, 0.050347082, 0.050153557, 0.050017543, 0.05014797, 0.049967747, 0.04981736, 0.049580254, 0.049521208, 0.049450144, 0.049351454, 0.049112543, 0.049232233, 0.049114607, 0.048853647, 0.048807837, 0.048850156]\n",
      "acc train: [0.9331229, 0.94235814, 0.9424873, 0.9461842, 0.95521116, 0.966293, 0.97348684, 0.9770506, 0.97884905, 0.9800555, 0.9809362, 0.9816518, 0.9822499, 0.9827117, 0.98313, 0.9834592, 0.98373586, 0.9839953, 0.98422295, 0.9844281, 0.9845946, 0.9847555, 0.9848799, 0.9850398, 0.9851617, 0.98526645, 0.98538977, 0.98545927, 0.9855491, 0.98564464, 0.9856998, 0.98575246, 0.9858209, 0.9858633, 0.98593146, 0.9859588, 0.98602194, 0.98605686, 0.98611945, 0.986128, 0.9861962, 0.9862123, 0.98624045, 0.98629403, 0.98632073, 0.98634374, 0.9863855, 0.9863995, 0.98644185, 0.98649067, 0.98650444, 0.9865307, 0.9865581, 0.9865839, 0.9866117, 0.98663914, 0.98666936, 0.986685, 0.98671836, 0.98674726]\n",
      "acc val: [0.9432055, 0.9432071, 0.95011884, 0.9644337, 0.9732217, 0.9787728, 0.98266405, 0.9843543, 0.9852675, 0.9857744, 0.9862508, 0.9867111, 0.98695, 0.98722947, 0.98737454, 0.9875017, 0.9875913, 0.9877246, 0.9877989, 0.9878659, 0.98793083, 0.9879927, 0.9880351, 0.9880871, 0.98813885, 0.988159, 0.98820496, 0.98824084, 0.98825157, 0.9882865, 0.988307, 0.9883252, 0.988331, 0.9883452, 0.9883631, 0.98841345, 0.9883934, 0.98840445, 0.9884344, 0.9884612, 0.98845786, 0.9884759, 0.98849595, 0.9885055, 0.98855233, 0.98857665, 0.9885837, 0.98856544, 0.9885977, 0.98863703, 0.9886516, 0.98865265, 0.9886514, 0.98865825, 0.988718, 0.98868114, 0.9887263, 0.98875684, 0.988738, 0.9887309]\n",
      "cols_label: ['epoch_1', 'epoch_2', 'epoch_3', 'epoch_4', 'epoch_5', 'epoch_6', 'epoch_7', 'epoch_8', 'epoch_9', 'epoch_10', 'epoch_11', 'epoch_12', 'epoch_13', 'epoch_14', 'epoch_15', 'epoch_16', 'epoch_17', 'epoch_18', 'epoch_19', 'epoch_20', 'epoch_21', 'epoch_22', 'epoch_23', 'epoch_24', 'epoch_25', 'epoch_26', 'epoch_27', 'epoch_28', 'epoch_29', 'epoch_30', 'epoch_31', 'epoch_32', 'epoch_33', 'epoch_34', 'epoch_35', 'epoch_36', 'epoch_37', 'epoch_38', 'epoch_39', 'epoch_40', 'epoch_41', 'epoch_42', 'epoch_43', 'epoch_44', 'epoch_45', 'epoch_46', 'epoch_47', 'epoch_48', 'epoch_49', 'epoch_50', 'epoch_51', 'epoch_52', 'epoch_53', 'epoch_54', 'epoch_55', 'epoch_56', 'epoch_57', 'epoch_58', 'epoch_59', 'epoch_60']\n",
      "rows_label: ['AE_loss', 'AE_val_loss', 'AE_acc', 'AE_val_acc']\n",
      "test human: (501, 27208)\n",
      "test_X_onehot: (501, 27208, 4)\n",
      "test_X_missing: (501, 27208, 4)\n",
      "test sample: 0\n",
      "0/501, sample ID: HG03718, accuracy: 0.9432\n",
      "test sample: 50\n",
      "50/501, sample ID: HG03311, accuracy: 0.9482\n",
      "test sample: 100\n",
      "100/501, sample ID: HG03021, accuracy: 0.9430\n",
      "test sample: 150\n",
      "150/501, sample ID: HG00475, accuracy: 0.9518\n",
      "test sample: 200\n",
      "200/501, sample ID: HG03604, accuracy: 0.9202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test sample: 250\n",
      "250/501, sample ID: HG02332, accuracy: 0.9353\n",
      "test sample: 300\n",
      "300/501, sample ID: HG02699, accuracy: 0.9410\n",
      "test sample: 350\n",
      "350/501, sample ID: HG02611, accuracy: 0.9568\n",
      "test sample: 400\n",
      "400/501, sample ID: NA19740, accuracy: 0.9403\n",
      "test sample: 450\n",
      "450/501, sample ID: HG04153, accuracy: 0.9412\n",
      "test sample: 500\n",
      "500/501, sample ID: HG01686, accuracy: 0.9351\n",
      "predict_onehot: (501, 27208, 4)\n",
      "predict_data: (501, 27208)\n",
      "df_data shape: (27208, 501)\n",
      "Beagle R2.\n",
      "Beagle R2.\n",
      "predict_onehot: (501, 27208, 4)\n",
      "predict_onehot_t: (27208, 501, 4)\n",
      "snp_cnt: 27208\n",
      "sample_cnt: 501\n",
      "mask_constant_cnt: 12795\n",
      "R2 mask_valid_cnt: 16021\n",
      "R2 mask_valid_perc: 0.5888341664216407\n",
      "R2_valid_perc: 0.5888341664216407\n",
      "AE avt R2: 0.7026603973490323\n",
      "PCC: squared pearson correlation coefficient\n",
      "rows: 501\n",
      "cols: 27208\n",
      "pcc_na_cnt: 11869\n",
      "pcc_non-na_cnt: 15339\n",
      "pcc rp_valid: 11475\n",
      "pcc rp_valid ratio: 0.7480930960297282\n",
      "pcc_valid_perc: 0.7480930960297282\n",
      "AE avt pcc: 0.8071992238877398\n",
      "Hellinger score\n",
      "predict_onehot: (501, 27208, 4)\n",
      "predict_onehot_t: (27208, 501, 4)\n",
      "snp_cnt: 27208\n",
      "sample_cnt: 501\n",
      "test_X_onehot_t: (27208, 501, 4)\n",
      "hellinger_score_mean mean: 0.948321521398536\n",
      "hellinger_score_min mean: 0.5153449508344021\n",
      "hellinger_score_mean mask_valid_cnt: 27208\n",
      "hellinger_score_mean mask_valid_perc: 1.0\n",
      "hellinger_score_min mask_valid_cnt: 17695\n",
      "hellinger_score_min mask_valid_perc: 0.6503601881799471\n",
      "hellinger_valid_perc: 1.0 0.6503601881799471\n",
      "SEN  score\n",
      "predict_onehot: (501, 27208, 4)\n",
      "predict_onehot_t: (27208, 501, 4)\n",
      "snp_cnt: 27208\n",
      "sample_cnt: 501\n",
      "test_X_onehot_t: (27208, 501, 4)\n",
      "sen_score_mean mean: 0.9950775882813043\n",
      "sen_score_min mean: 0.8302172432273613\n",
      "sen_mean: 0.9950775882813043 0.8302172432273613\n",
      "IQS\n",
      "predict_onehot: (501, 27208, 4)\n",
      "data_imp_012: (27208, 501)\n",
      "snp_cnt: 27208\n",
      "sample_cnt: 501\n",
      "test_X_onehot_t: (27208, 501, 4)\n",
      "iqs mean: 0.9021452050439849\n",
      "iqs_mean: 0.9021452050439849\n",
      "avg_miss: []\n",
      "avg_miss ratio: nan\n",
      "rows_label: ['accuracy']\n",
      "rows_label: ['r2', 'pcc', 'hen', 'hen_min', 'sen', 'sen_min', 'iqs']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ihome/msong/anaconda3/envs/keras_torch/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/ihome/msong/anaconda3/envs/keras_torch/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avt acc: 0.9442306344854859\n",
      "test cols_label: ['avg_metrics']\n",
      "rows_label: ['avg_accuracy', 'avg_r2', 'avg_pcc', 'avg_hel', 'avg_hel_min', 'avg_sen', 'avg_sen_min', 'avg_iqs']\n",
      "timestr ['2022/09/27', '21:43:33']\n"
     ]
    }
   ],
   "source": [
    "# base version v2: sigle vae for snp \n",
    "\n",
    "# 1.3 train snp vae only\n",
    "from tensorflow.keras.losses import mean_squared_error, categorical_crossentropy\n",
    "\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "# l1 distance\n",
    "from scipy.spatial.distance import cityblock\n",
    "\n",
    "\n",
    "#run_times = 2\n",
    "\n",
    "\n",
    "\n",
    "for k in range(run_times):\n",
    "    print(\"#**********************************************************************#:\")\n",
    "    print(\"start of k_th time:\", k)\n",
    "    \n",
    "    # set random seed for split\n",
    "    print(\"start of k_th seed:\", k, random_seed[k])\n",
    "    #np.random.seed(seed=28213)\n",
    "    np.random.seed(seed=random_seed[k])\n",
    "    \n",
    "    \n",
    "    # split into training/valid and test data for HLA, U19 data, 1000G except yeast data\n",
    "    # for yeast data, skip it.\n",
    "    \n",
    "    \n",
    "    # if (data_type == 'Human')\n",
    "    df_ori, test_X = train_test_split(snp_new_t, test_size=0.2)\n",
    "    print(\"df_ori shape:\", df_ori.shape)\n",
    "    print(\"test_X shape:\", test_X.shape)\n",
    "    # else: skip for yeast\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # one hot encode\n",
    "    df_onehot = to_categorical(df_ori)\n",
    "    #df_onehot.shape\n",
    "    print(\"df_onehot.shape:\", df_onehot.shape)\n",
    "\n",
    "    # Further split df_ori to train and valid\n",
    "    train_X, valid_X = train_test_split(df_onehot, test_size=0.2)\n",
    "    #train_X.shape, valid_X.shape\n",
    "    print(\"train_X.shape:\", train_X.shape)\n",
    "    print(\"valid_X.shape:\", valid_X.shape)\n",
    "    \n",
    "    \n",
    "    # fake missing values\n",
    "    train_X_fake = generate_fake_missing(train_X, missing_ratio)\n",
    "\n",
    "    diff = np.absolute(np.array(train_X) - np.array(train_X_fake))\n",
    "    print('train_X_fake diff:', np.sum(diff))\n",
    "\n",
    "\n",
    "    valid_X_fake = generate_fake_missing(valid_X, missing_ratio)\n",
    "    diff = np.absolute(np.array(valid_X) - np.array(valid_X_fake))\n",
    "    print('valid_X_fake diff:', np.sum(diff))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    snp_ae = SNP_AE(feature_size)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "    train_loss_metric = tf.keras.metrics.CategoricalCrossentropy()\n",
    "    val_loss_metric = tf.keras.metrics.CategoricalCrossentropy()\n",
    "\n",
    "\n",
    "    train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "    val_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "\n",
    "    loss_vae = []\n",
    "\n",
    "    loss_train = []\n",
    "    loss_val = []\n",
    "\n",
    "    acc_train = []\n",
    "    acc_val = []\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('Start of epoch %d' % (epoch,))\n",
    "\n",
    "        #----shuffle train data and lablel for each epoch-----------------------------------------------------------------------------------#    \n",
    "        # shuffle data and labels at the same time\n",
    "        idx = train_X.shape[0]\n",
    "\n",
    "\n",
    "        indices = tf.range(start=0, limit=idx, dtype=tf.int32)\n",
    "        #print('indices:', indices)\n",
    "\n",
    "        shuffled_indices = tf.random.shuffle(indices)  \n",
    "      \n",
    "        train_X_fake = tf.gather(train_X_fake, shuffled_indices)\n",
    "        train_X = tf.gather(train_X, shuffled_indices)        \n",
    "        \n",
    "        snp_x = tf.data.Dataset.from_tensor_slices(train_X_fake).batch(bs, drop_remainder=True) \n",
    "        snp_y = tf.data.Dataset.from_tensor_slices(train_X).batch(bs, drop_remainder=True)\n",
    "\n",
    "        snp_x_v = tf.data.Dataset.from_tensor_slices(valid_X_fake).batch(bs, drop_remainder=True)\n",
    "        snp_y_v = tf.data.Dataset.from_tensor_slices(valid_X).batch(bs, drop_remainder=True)\n",
    "\n",
    "        loss_batch = []\n",
    "\n",
    "        # Iterate over the batches of the dataset.\n",
    "        for step, (snp_fake_batch, snp_label_batch) in enumerate(zip(snp_x, snp_y)): \n",
    "            with tf.GradientTape() as tape:\n",
    "                recon_inputs, latents= snp_ae(snp_fake_batch, training=True)\n",
    "                loss = loss_function_cce(recon_inputs, snp_label_batch)\n",
    "\n",
    "\n",
    "            grads = tape.gradient(loss, snp_ae.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, snp_ae.trainable_variables))    \n",
    "\n",
    "            loss_batch.append(loss.numpy())\n",
    "\n",
    "\n",
    "            # Update training metric.\n",
    "            train_acc_metric.update_state(snp_label_batch, recon_inputs)\n",
    "            train_loss_metric.update_state(snp_label_batch, recon_inputs)\n",
    "\n",
    "\n",
    "        # Display metrics at the end of each epoch.\n",
    "        train_loss = train_loss_metric.result()\n",
    "        print(\"Training loss over epoch: \", epoch, train_loss.numpy())\n",
    "\n",
    "        train_acc = train_acc_metric.result()\n",
    "        print(\"Training acc over epoch: \", epoch, train_acc.numpy())\n",
    "\n",
    "\n",
    "        loss_train.append(train_loss.numpy())\n",
    "        acc_train.append(train_acc.numpy())\n",
    "\n",
    "\n",
    "        # Reset training metrics at the end of each epoch\n",
    "        train_loss_metric.reset_states()\n",
    "        train_acc_metric.reset_states()\n",
    "\n",
    "\n",
    "        # Run a validation loop at the end of each epoch.\n",
    "        for x_batch_val, y_batch_val in zip(snp_x_v, snp_y_v):   \n",
    "\n",
    "            val_recons, latents = snp_ae(x_batch_val, training=False)\n",
    "            # Update val metrics\n",
    "            val_loss_metric.update_state(y_batch_val, val_recons)\n",
    "            val_acc_metric.update_state(y_batch_val, val_recons)\n",
    "\n",
    "        val_loss = val_loss_metric.result()\n",
    "        val_acc = val_acc_metric.result()\n",
    "\n",
    "        loss_val.append(val_loss.numpy())\n",
    "        acc_val.append(val_acc.numpy())\n",
    "\n",
    "        val_loss_metric.reset_states()\n",
    "        val_acc_metric.reset_states()\n",
    "        #print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "        print(\"Validation loss: \", epoch, val_loss.numpy())\n",
    "        print(\"Validation acc: \", epoch, val_acc.numpy())\n",
    "\n",
    "\n",
    "        #print('epoch %s: batch loss = %s' % (epoch, loss_batch))\n",
    "        loss_epoch = np.mean(loss_batch)    \n",
    "        print('epoch %s: loss = %s' % (epoch, loss_epoch))    \n",
    "\n",
    "        loss_vae.append(loss_epoch)    \n",
    "\n",
    "    print('loss_vae:', loss_vae) \n",
    "\n",
    "    print('loss train:', loss_train)     \n",
    "    print('loss val:', loss_val)     \n",
    "\n",
    "    print('acc train:', acc_train)     \n",
    "    print('acc val:', acc_val) \n",
    "\n",
    "    #write out loss and accuracy into excels\n",
    "    loss2 = loss_train\n",
    "    val_loss2 = loss_val\n",
    "\n",
    "    acc2 = acc_train\n",
    "    val_acc2 = acc_val\n",
    "    \n",
    "    #ae_loss = [loss1, val_loss1, acc1, val_acc1]\n",
    "    ae_loss = [loss2, val_loss2, acc2, val_acc2]\n",
    "\n",
    "\n",
    "    file_name = 'ae_hla_loss_acc_v'+str(k)+'_missing_ratio_'+str(missing_perc)+'.xlsx'\n",
    "    file_path_out = os.path.join(data_path, file_name)\n",
    "\n",
    "    cols_label = [0 for i in range(epochs)]\n",
    "\n",
    "    for i in range(epochs):\n",
    "        cols_label[i] = 'epoch_'+str(i+1)\n",
    "\n",
    "    print('cols_label:', cols_label)    \n",
    "\n",
    "\n",
    "    rows_label = ['AE_loss', 'AE_val_loss', 'AE_acc', 'AE_val_acc']\n",
    "    print('rows_label:', rows_label)    \n",
    "\n",
    "    df = pd.DataFrame(data = np.array(ae_loss), index = rows_label, columns = cols_label)\n",
    "    df.to_excel(file_path_out, na_rep='NA', header=True, index=True, float_format=\"%.6f\")  \n",
    "\n",
    "    # load test data \n",
    "#     #1.0 test yeast data\n",
    "#     input_name = '/ihome/msong/u19/data/yeast_genotype_test.txt'\n",
    "#     df_ori = pd.read_csv(input_name, sep='\\t', index_col=0)\n",
    "#     print('test yeast:', df_ori.shape)\n",
    "\n",
    "\n",
    "    #2.0 test human data: HLA, U19, 1000G\n",
    "    df_ori = test_X\n",
    "    #df_ori.shape \n",
    "    print('test human:', df_ori.shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # one hot encode\n",
    "    test_X_onehot = to_categorical(df_ori)\n",
    "    #test_X_onehot.shape\n",
    "    print('test_X_onehot:', test_X_onehot.shape)\n",
    "\n",
    "    #test_X_missing = test_X.copy()\n",
    "    test_X_missing = test_X_onehot[:, 0:feature_size, :].copy()\n",
    "\n",
    "    #test_X_missing.shape\n",
    "    print('test_X_missing:', test_X_missing.shape)\n",
    "    \n",
    "    \n",
    "    # test evaluation\n",
    "    avg_accuracy = []\n",
    "    avg_rmse = []\n",
    "    avg_l1 = []\n",
    "    avg_cos_thelta = []\n",
    "\n",
    "    \n",
    "    avg_miss = []\n",
    "        \n",
    "    log_freq = 50\n",
    "    \n",
    "    \n",
    "    for i in range(test_X_missing.shape[0]):\n",
    "        if (i % log_freq ==0):\n",
    "            print('test sample:', i)\n",
    "        \n",
    "        # Generates missing genotypes\n",
    "        missing_size = int(missing_perc * test_X_missing.shape[1])\n",
    "        \n",
    "        \n",
    "        # without repeat random numbers: set replace with false\n",
    "        missing_index = np.random.choice(test_X_missing.shape[1], size=missing_size, replace=False)\n",
    "\n",
    "        \n",
    "        if(missing_perc != 0):\n",
    "            #test_X_missing[i, missing_index, :] = [1, 0, 0]  # yeast\n",
    "            test_X_missing[i, missing_index, :] = [1, 0, 0, 0]  # human\n",
    "\n",
    "            \n",
    "\n",
    "        # AE\n",
    "\n",
    "        predict_onehot, latent_test = snp_ae(test_X_missing[i:i + 1, :, :], training=False)\n",
    "        predict_onehot = predict_onehot.numpy()\n",
    "\n",
    "\n",
    "        # only care the missing position\n",
    "        if(missing_perc == 0):\n",
    "            predict_missing_onehot = predict_onehot[0:1, :, :]\n",
    "        else:\n",
    "            predict_missing_onehot = predict_onehot[0:1, missing_index, :]\n",
    "\n",
    "\n",
    "        # predict label\n",
    "        predict_missing = np.argmax(predict_missing_onehot, axis=2)\n",
    "\n",
    "        # real label\n",
    "\n",
    "        if(missing_perc == 0):\n",
    "            label_missing_onehot = test_X_onehot[i:i + 1, :, :]\n",
    "        else:\n",
    "            label_missing_onehot = test_X_onehot[i:i + 1, missing_index, :]\n",
    "\n",
    "\n",
    "\n",
    "        label_missing = np.argmax(label_missing_onehot, axis=2)\n",
    "\n",
    "        \n",
    "        # accuracy\n",
    "        correct_prediction = np.equal(predict_missing, label_missing)\n",
    "        accuracy = np.mean(correct_prediction)\n",
    "        \n",
    "        if (i % log_freq ==0):\n",
    "            print('{}/{}, sample ID: {}, accuracy: {:.4f}'.format(\n",
    "                i, test_X_missing.shape[0], df_ori.index[i], accuracy))\n",
    "\n",
    "        avg_accuracy.append(accuracy)\n",
    "\n",
    "\n",
    "    ###############################################################\n",
    "\n",
    "    \n",
    "    # write out test of 1kg\n",
    "    predict_onehot, latent_test = snp_ae(test_X_missing[:, :, :], training=False)\n",
    "    \n",
    "    print('predict_onehot:', predict_onehot.shape)\n",
    "\n",
    "    # save imputed LOS output\n",
    "\n",
    "    file_name = 'AE_hla_imputed_v'+str(k)+'_missing_ratio_'+str(missing_perc)+'.csv'\n",
    "    file_path_out = os.path.join(data_path, file_name)\n",
    "    \n",
    "    data = np.argmax(predict_onehot, axis=2)\n",
    "    print('predict_data:', data.shape)\n",
    "    \n",
    "    data = data - 1\n",
    "    \n",
    "    # transpose\n",
    "    df_data = data.transpose()\n",
    "    print(\"df_data shape:\", df_data.shape)\n",
    "    \n",
    "    df = pd.DataFrame(data = np.array(df_data), columns = df_ori.index)\n",
    "    df.insert(loc=0, column='snp_pos', value=snp_rows_label.values)\n",
    "    df.to_csv(file_path_out, na_rep='NA', header=True, index=False)      \n",
    "    \n",
    "    \n",
    "     ###############################################################\n",
    "        \n",
    "    # calculate metrics: 6/6/2022\n",
    "    \n",
    "    #data_imp = SCDA.predict(test_X_missing[:, :, :])\n",
    "    data_imp_ae, latent_test = snp_ae(test_X_missing[:, :, :], training=False)\n",
    "    data_imp = np.array(data_imp_ae)\n",
    "    \n",
    "    data_obs = test_X_onehot\n",
    "\n",
    "    \n",
    "    # 5.30.2022, Calculate Beagle R2 (without ground truth)\n",
    "    print('Beagle R2.')\n",
    "        \n",
    "    R2_a, R2_valid_perc_a = beagle_r2(data_imp)\n",
    "    print('R2_valid_perc:', R2_valid_perc_a)\n",
    "    \n",
    "    print('AE avt R2:', np.mean(R2_a))\n",
    "    \n",
    "    \n",
    "    ###############################################################   \n",
    "    \n",
    "    # 6/2/2022\n",
    "    # PCC: squared pearson correlation coefficient\n",
    "    print('PCC: squared pearson correlation coefficient') \n",
    "    \n",
    "    r_a, pcc_valid_perc_a = pcc(data_imp, data_obs)\n",
    "    print('pcc_valid_perc:', pcc_valid_perc_a)\n",
    "    \n",
    "    #print('AE avt pcc:', np.mean(r_a))\n",
    "    print('AE avt pcc:', np.nanmean(r_a))\n",
    "    \n",
    "    \n",
    "    ###############################################################   \n",
    "  \n",
    "    # 6/4/2022\n",
    "    # Hellinger score\n",
    "    # for each snp, calculate the mean (sd, minimum, maximum, median, quartiles) \n",
    "    # of hellinger score across samples\n",
    "    \n",
    "    print('Hellinger score')\n",
    "    \n",
    "    \n",
    "    hellinger_score_mean_a, hellinger_score_min_a, hellinger_valid_perc_mean_a, hellinger_valid_perc_min_a \\\n",
    "    = hellinger_score(data_imp, data_obs)\n",
    "    \n",
    "    print('hellinger_valid_perc:', hellinger_valid_perc_mean_a, hellinger_valid_perc_min_a)\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "    ###############################################################   \n",
    "    # 6/4/2022\n",
    "    # SEN score: Scaled Euclidian Norm Score\n",
    "    # for each snp, calculate the mean (sd, minimum, maximum, median, quartiles) \n",
    "    # of SEN score across samples\n",
    "    \n",
    "    print('SEN  score')\n",
    "    \n",
    "    \n",
    "    sen_score_mean_a, sen_score_min_a, sen_score_mean_mean_a, sen_score_min_mean_a \\\n",
    "    = sen_score(data_imp, data_obs)\n",
    "    \n",
    "    print('sen_mean:', sen_score_mean_mean_a, sen_score_min_mean_a)\n",
    "    \n",
    "\n",
    "    ###############################################################   \n",
    "    # 6/4/2022\n",
    "    # IQS: imputation quality score\n",
    "    # for each snp, calculate IQS across samples \n",
    "    print('IQS')\n",
    "        \n",
    "    iqs_a, iqs_mean_a = iqs_score(data_imp, data_obs)\n",
    "    \n",
    "    print('iqs_mean:', iqs_mean_a)\n",
    "    \n",
    "    \n",
    "        \n",
    "    print('avg_miss:', avg_miss)\n",
    "    print('avg_miss ratio:', np.mean(avg_miss))\n",
    "    \n",
    "    ae_metrics = [avg_accuracy]\n",
    "\n",
    "\n",
    "    file_name = 'ae_hla_metrics_acc_v'+str(k)+'_missing_ratio_'+str(missing_perc)+'.csv'\n",
    "    file_path_out = os.path.join(data_path, file_name)\n",
    "\n",
    "\n",
    "    cols_label = [0 for i in range(test_X_missing.shape[0])]\n",
    "\n",
    "    for i in range(test_X_missing.shape[0]):\n",
    "        cols_label[i] = 'sample_'+ str(df_ori.index[i])\n",
    "\n",
    "    rows_label = ['accuracy']\n",
    "    print('rows_label:', rows_label)    \n",
    "\n",
    "    df = pd.DataFrame(data = np.array(ae_metrics), index = rows_label, columns = cols_label)\n",
    "    df.to_csv(file_path_out, na_rep='NA', header=True, index=True, float_format=\"%.6f\") \n",
    "    \n",
    "    \n",
    "    # 7/14/2022 write out r2, pcc, hen, sen, iqs\n",
    "    scda_metrics = [R2_a, r_a, hellinger_score_mean_a, hellinger_score_min_a, \n",
    "                    sen_score_mean_a, sen_score_min_a, iqs_a]\n",
    "\n",
    "\n",
    "    file_name = 'ae_hla_metrics_new_v'+str(k)+'_missing_ratio_'+str(missing_perc)+'.csv'\n",
    "    file_path_out = os.path.join(data_path, file_name)\n",
    "\n",
    "    \n",
    "    cols_label = snp_rows_label.values.ravel() \n",
    "\n",
    "    rows_label = ['r2', 'pcc', 'hen', 'hen_min', 'sen', 'sen_min', 'iqs']\n",
    "    \n",
    "    print('rows_label:', rows_label)    \n",
    "\n",
    "    \n",
    "    df = pd.DataFrame(data = np.array(scda_metrics), columns = cols_label)\n",
    "    df.insert(loc=0, column='metrics', value=rows_label)\n",
    "    \n",
    "    \n",
    "    df.to_csv(file_path_out, na_rep='NA', header=True, index=False, float_format=\"%.6f\")   \n",
    "\n",
    "\n",
    "    \n",
    "    # write out average metrics over samples\n",
    "    print('avt acc:', np.mean(avg_accuracy))    \n",
    "\n",
    "    scda_metrics_avg = [np.mean(avg_accuracy), np.mean(R2_a), np.nanmean(r_a), \n",
    "                        np.mean(hellinger_score_mean_a), np.mean(hellinger_score_min_a),\n",
    "                        np.mean(sen_score_mean_a), np.mean(sen_score_min_a), iqs_mean_a]\n",
    "\n",
    "\n",
    "    file_name = 'ae_hla_metrics_avg_v'+str(k)+'_missing_ratio_'+str(missing_perc)+'.xlsx'\n",
    "    file_path_out = os.path.join(data_path, file_name)\n",
    "        \n",
    "    \n",
    "    cols_label = ['avg_metrics']\n",
    "    print('test cols_label:', cols_label)    \n",
    "\n",
    "    rows_label = ['avg_accuracy','avg_r2','avg_pcc','avg_hel','avg_hel_min','avg_sen',\n",
    "                 'avg_sen_min','avg_iqs']\n",
    "    \n",
    "    print('rows_label:', rows_label)    \n",
    "\n",
    "    df = pd.DataFrame(data = np.array(scda_metrics_avg), index = rows_label, columns = cols_label)\n",
    "    df.to_excel(file_path_out, na_rep='NA', header=True, index=True, float_format=\"%.6f\")  \n",
    "\n",
    "      \n",
    "    \n",
    "# add timestamp\n",
    "import time\n",
    "\n",
    "timestr1 = time.strftime(\"%Y/%m/%d\")\n",
    "timestr2 = time.strftime(\"%H:%M:%S\")\n",
    "timestr = [timestr1, timestr2]\n",
    "print('timestr', timestr)\n",
    " \n",
    "file_name = 'ae_hla_log_v'+str(k)+'_missing_ratio_'+str(missing_perc)+'.csv'\n",
    "file_path_out = os.path.join(data_path, file_name)\n",
    "    \n",
    "    \n",
    "rows_label = ['day', 'time']\n",
    "cols_label = ['value']\n",
    "  \n",
    "df = pd.DataFrame(data = np.array(timestr), columns = cols_label)\n",
    "df.insert(loc=0, column='Time_log', value=rows_label)\n",
    "\n",
    "    \n",
    "df.to_csv(file_path_out, na_rep='NA', header=True, index=False)     \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#**************************************************************************#\n",
      "plot i_th fig: 1\n",
      "loss1 start: [0.344375, 0.226658, 0.181228, 0.153037, 0.132845]\n",
      "loss1 end: []\n",
      "val_loss1 start: [0.240919, 0.193932, 0.151426, 0.129595, 0.107857]\n",
      "val_loss1 end: []\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc1 start: [0.933123, 0.942358, 0.942487, 0.946184, 0.955211]\n",
      "acc1 end: []\n",
      "val_acc1 start: [0.943205, 0.943207, 0.950119, 0.964434, 0.973222]\n",
      "val_acc1 end: []\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAqRUlEQVR4nO3de5xU9X3/8deHZWEFURBWRCBADCoQbkKIQlSUaNBWjRqi1hhN0phfG/NLm5gUE8vP0Frb1FzMryYNaax3iSUxamKiAUGN5sIiYuRmEFAWBJZVkKvssp/+8T3Dnh1mdmd3Z3Z2zr6fj8d5nJlzm++ZOfOe73zPme+YuyMiIsnVrdgFEBGRwlLQi4gknIJeRCThFPQiIgmnoBcRSTgFvYhIwinopdMxszPNbG0z84ebmZtZ9wI9/kozm96O9X9lZtfmr0T5ZWZ7zOy9Bdju35jZtmj7/fO9/Y5kZl8zs/8qdjnyJXFBb2ZLzOxtM+uZNv1uMzsYHYSpYUWWbUw3s+os2/7r5pbJsB03s39ozz51Ne7+nLufkrpvZhvN7MMd+Phj3H1JO9a/wN3vyWOR2ix+zKa4+9Huvj7Pj1MOfBs4P9p+bYZlJpjZMjPbF40nZNlWTzP7sZm9bma7zewlM7ugHWVr9vjJ9F52939x97/Otk4+mdk8M1trZg1mdl0hHiNRQW9mw4EzAQcuzrDIN6ODMDWML3CRrgXeAj5Z4Mdpl0LVjKWwOtnrNhCoAFZmmmlmPYBHgfuBfsA9wKPR9HTdgU3A2cCxwM3Aw9H7O4lWAH8LvFiwR3D3xAzAHOB5Qs3iF2nz7gb+OcftTAeqM0xfAvx1c8vElu0N7AauBA4Ck5tZth/wC6AGeDu6PSQ2/zjgv4Et0fyfx+ZdArwEvAO8BsyMpm8EPhxb7hbg/uj2cMKH4WeAN4Bno+n/A2wFdgHPAmNi6x8FfAt4PZr/22jaL4EvpO3Py8ClGfbzHuDL0e3BURk+H90/ifCh2C3+3AL3AQ3AfmAP8NVY+a+Nyr8D+Hozz+/dwPeBX0XbeB44Afhu9HyuASbGlj/83AFTgKro+d0GfDuaXkEIrVpgJ7AUGJjhOLkueq5ujx5rA3BB7LFGRM/1bmAhcGfqdcp2XAL/EL1O99HMsQPcChwCDkT7/R/RdAfeF90+Frg3Wv91Qqh2y/L4PaPnbEs0fDeadjKwN9ruHuDpDOueD2wGLDbtDaLjNYf35MvA5VnmnQQ8Hb0WO4AHgL7Zjp8M79P90TJ7ouFEMr9fPkX4AHob+D/AB6Jy7Uw9t7HtfhpYHS37JDAsh338LXBdW/OvuSFRNXpCzfmBaPiImQ0sYlkuIxw0/0N4oZtrs+1GCPJhwHsIB95/xObfB/QCxgDHA98BMLMphDfpV4C+wFmEkMrV2cAo4CPR/V8BI6PHeJHwPKbcDkwCphI+eL5KeHPcA3witZCZjSeE+C8zPN4zhLBKPfb6qMyp+8+5e0N8BXe/hhAIF3n4FvbN2OwPAacAM4A5ZjaqmX39OCHEBgDvAr+L9nEAsIBQOcjkDuAOdz+GECgPR9OvJYTkUKA/4Y2/P8s2PgisjR7rm8CPzcyieQ8Cf4y2cQtwTTP7AOED6jjCsXI9zRw77v514Dnghui5uyHD9v5/tB/vJbwGnyQEWiZfB04HJgDjCR+CN7v7q4RjE0LAnpth3THAyx4lWuTl2HpZRe/jk8nybQEw4DZCQI8ivCa3QIvHD+6+F7gA2OKN3/S3ZHmcDxLeH1cQPuS+Dnw42oePm9nZUXkvAb5GyIBKwmvwUEv7WVCF+PQoxkB409cBA6L7a4C/j82/m1Cz2Rkb7smyremEENuZNtSTe41+IfDd6PZVhBpTeY77MgF4O7o9KCpLvwzL/RD4TpZtbKTlGv17mylD32iZYwlhsh8Yn2G5CkKtZWR0/3bg+1m2eVK0bDfgP4HP0Vhzvwf4UqbnNsO+pMof/9bzR+DKLI97N/Cj2P0vAKtj98cCOzM9HqG2/Y3UcRVb5tPAC8C4DI+3hKY1+nWxeb2isp9ACOZ6oFds/v00X6M/CFTkcuyklyU2zYH3AWXR9kbH5n0OWJJl268BF8bufwTYmPaadM+y7j8C89OmPQDc0sJ7oZzwXvphLu+daJ2PAsuzHT9ZntfqtGm3cOT7ZXBsfi1wRez+T4G/i27/CvhMbF43YB8t1OpRjT4n1wJPufuO6P6DHFmLvt3d+8aG5mrZW9KW7Ut4IVpkZkOBc2isET9KCMS/yLJ8LzP7YXTy6R1CuPQ1szJC7eQtd387w6pDCW++ttoUK0OZmf2rmb0WlWFjNGtANFRkeix3PwD8BPiEmXUjfKjdl+nB3P01wlf8CYRzKb8AtpjZKYTa5DOtLP/W2O19wNHNLLstdnt/hvvZ1v0MoTa5xsyWmtlfRtPvI3xTm29mW8zsm9EJyWbL6e77optHE2qgb8WmQew1yaImes6BFo+dlgwgBOnrsWmvE76RZXJihmVPzOFxIHy7PSZt2jGEJquMouPpPsKHUaZvI6nlBprZfDPbHD0H9xP2Ld9yPYaGAXeY2U4z20lokjSyP68Fl4igN7OjCF/NzzazrWa2Ffh7YHzUlNDRriE8t49HZVlPCMpsHyxfJjRBfNBDE0GqOcMIb/zjzKxvhvU2EWrJmewl1B5TTsiwTPxr9F8R2vs/TKjFD4+VYQfh21C2x7oHuJrQhLLP3X+XZTkIYf4xoIe7b47uX0toa34pyzqeZXrBufuf3f0qQnPWvwELzKy3u9e5+zfcfTShOesvaf1J9zcJr238dRraUpHS7jd37GRaPm4H4VvwsNi09xDa0jPZkmHZbM0c6VYC42JNVgDjyH7y1oAfE07yXu7udc1s+18I+zk2eg4+QeP+Q8vHT76Pr03A59Iqike5+wt5fpycJSLoCV/VDgGjCbXFCYS2uuco8BUvZlaRNhghuL4RK8sE4HLgwizXF/ch1Ah2mtlxwP9LzXD3NwlfBb9vZv3MrNzMUm/mHwOfMrMZZtbNzAab2anRvJeAK6PlJxPCtTl9CG3XtYQPiH+JlaEBuAv4tpmdGNX+z0hdwhoFewPhZG3G2nzMM4Ta2bPR/SXR/d+6+6Es62wjtCF3ODP7hJlVRs/Bzmhyg5mdY2Zjo5rzO4TAbMi2nUzc/XXCid5bzKyHmZ0BXNTKImY9diJZn7vo+X4YuNXM+pjZMOBLhBpxJg8BN5tZpZkNIFz8kG3ZdEsI79H/G10+maqhP51l+R8Q3sMXuXu2cx8pfQjfGHaZ2WDCOau4lo6fbUB/Mzu2hcfJ1X8CN5nZGAAzO9bMZmVbOHrtKwgfTuVRjuQ1m5MS9NcC/+3ub7j71tRAOCl1dewytK9a0+vod2TfZE4GE95k8WEaodZzZ7ws7v4YsI7QtJHuu4QrWHYAvwd+nTb/GkKQrAG2A38H4O5/JJw4+w7hSphnaKxx/SONbeLfIDRlNedewlfxzcCqqBxxNwJ/Ilxd8hahdtstbf2xtPzGf4bwxkwF/W8JHyzPZl0jnGi7OfoqfGML28+3mcBKM9tDODF7ZRQ8JxBO4r5DuLriGVr+kMvkauAMwgfsPxOawd5txfrfpflj5w7gYxZ+W/K9DOt/gfDtbz3htXiQ8KGeyT8TPpheJhwLL0bTWuTuBwkVsk8SPjA/DXw0mp76gdKvotvDCOcKJgBbY+/Xq7Ns/hvAaYT3wC+Bn6XNb/b4cfc1hA+x9dEyuTZHZdvXRwjvj/lRU9IrhBO+2TxFyI6pwLzo9lnNLN9qFp0EEGkXM/skcL27f6jYZSllZvYTYI27p9fMRdosKTV6KaKojflvCbURaQUz+4CZnRQ1vc0knCf5eZGLJQmjoJd2MbOPEC4d3UbLzUNypBMI7dd7gO8Bf+Puy4taIkkcNd2IiCScavQiIgnXmTpFAmDAgAE+fPjwYhdDRKSkLFu2bIe7V2aa1+mCfvjw4VRVVRW7GCIiJcXMXs82T003IiIJl1PQm9lMCx3jrzOz2RnmDzOzRWb2soU/OhgSm/dvZvZKNFyRz8KLiEjLWgz66CfedxJ+2TUauMrMRqctdjtwr7uPA+YSfomGmf0F4RdrEwhdfN5oZukdG4mISAHlUqOfQuhmdX30c+X5hB91xI2msc+KxbH5owl/alHvod/nlwk/KRcRkQ6SS9APpmnXqdUc2d3mCkIn+wCXAn2izrtWADOjrlQHELruPaJ3PjO73syqzKyqpqamtfsgIiLNyNfJ2BsJXQQvJ/Qrvhk45O5PAU8Q/qDhIcK/+hzRQ6G7z3P3ye4+ubIy49VBIiLSRrkE/Waa1sKHkNZftbtvcffL3H0i4e+1cPed0fhWd5/g7ucRuuF8NR8FFxGR3ORyHf1SYKSZjSAE/JWEP6k4LGqWeSvqs/smom5OoxO5fd291szGEf5o4Kk8ll9EOht3qKuDgwehvj4MdXVhfOhQmB8fzKCsLAzdu4cxhPVT2zl4EN59t3GcGg4eDNtsaAjj1O2WunZJ/f9JvByp9erqYO9e2LevcXzwIPToAT17No7LysL0Awcah1SZ4mWvq2vcx9T+de8O3bqF6enDAw+Ex8ijFoPe3eujPwl4kvAfk3e5+0ozmwtURf2sTwduMzMn9Cv++Wj1cuC56E9l3gE+4e71ed0DkXxzD6EUf8OmAisVVunj9JBJD5BUEMUDKX04eBD272861NU1DYdUIKa2GR/q6hqHVLlTZY/fjodgPPDS9yU1pO9rah9T40OHQsDt398YeOpDq+3ua8vfGjQvp1/GuvsThLb2+LQ5sdsLCH/CkL7eAcKVNyJNuTcGW6p2lqoRHTgQpu/bd2TwxYcDB0J4pUIoNaRvK3U7XiNM1TbjodXQ0Bjw0j7du4daaffuUF4exqkPqvQabOrDIv7BAmH9+FBeHmrS8SH1GN26hW1369Y4ZJP6EEp9mzBrWrsuL4fevaFXr8Zxjx5Hfquorw9lqKiAo44K41SZUuVNlQ+afmDGj730oTzbXw+34+XI+xYlmQ4ehNpaeOcd2L276bBnTxj27m28Hb+/d++Rt/fuDQd9Z5UKqnhYpWrVmWrYqbDJ9HU8HkLxcfrQo0cIjNRQURGmpdeqGxqaPlbqdnl5Y7ikbqdCNn47FYLpte74/qQ3pcTnZXrsioqmQ6r5RToFBX1X1NAAb70F27bB9u0hwHfuhF27wnjnzjB/69YwbNsWlsm37t2b1oTi41TY9erVNPzi9ysqmgZwKohSYZO+7fQaYjycU0MqcJurEYqUGAV90rhDdTWsWAGbNsGWLfDmm2G8ZUsI7Zqa1temy8pgwAA45hjo06fpcPTRYejdu+k4Pj0+L/WVOM8nnEQkMwV9KXv3XXj1VVi5EpYvbxx25PCf58cdB8cfH4YBA6Bv3zAce2wY9+sHJ5zQOPTvr1quSIlS0JeK+nr47W9h4UJYtSqE+2uvZa6Z9+sHEyfC+94HgwbBiSeGYdCgMAwYoNq0SBEtWgRz5sDcuTBjRvZpeePunWqYNGmSS2T/fvfHHnP/1Kfc+/c/8vx8t27uI0e6X3KJ+5w57o884r5xo3tDQ7FLLpJXCxe6T50axs1Na82y+Z7WmmWnTg1v4alTm5/WGoTL3TPmatGDPX3o8kG/Z4/7ww+7f/zj7r17Nw32kSPdv/xl9wcfdH/ppfBBINJOxQy8QgRjrsvme1prlm3Nc5krBX1n98477g895H755e5HHdU03E87zf2f/sn9lVdUU0+wYgZrMQOvEMFYCjX6QlDQd0b797v/7Gfus2a5V1Q0DfczznD/1rfcN2wodim7vFII2/auXwo1emmZgr4zee650OZ+zDFNw33aNPc77nDftKnYJUycUmgyKGawSjIo6DuDbdvcr766abhPnOj+zW+6v/56sUvXqbU38EqhyUCkvRT0xdTQ4P7jH7sfd1x4uisq3G+6yX316mKXrOg6qlatAJauQEFfLGvWuE+f3liDP+8893Xril2qomhPgLe3Vi3SFSjoi+GZZxpPslZWut9/fyKvmmlPrVy1apH8UdB3tFdfbWyqufJK99raYpeo3bIFcHtq5SKSP80FvTovybfaWrjwwtD740UXwf33h35lSsiiRTBtWhinzJkDL7wQxnFz58LUqWHc3LQZM+D55wvw024RaVm2T4BiDSVdoz9wwP1DH/LDV9Ts3l3sErWoPc0sItJ5oBp9B3CHT386dDw2eDA8/njolrcTybWmrhq5SLIo6PPlllvgwQdDuP/ylyHsOxmFukjXpKDPh4ULQ1J26wY/+QmMH1/sEmWsvSvURbomBX0+/OAHYXzzzeFEbAfLtUlGoS7SNSno26u2NrTHd+sGn/tcUYqQa5OMiHRNCvr2eughqKuD888P/+JUQJlq7qAmGRFpnoK+ve6+O4yvvbbgD5XtWnaFuog0R0HfHq+8AsuWhT/UvuSSgj+cmmNEpC0U9O1xzz1hfMUVcNRRed10pmYa1dxFpC1yCnozm2lma81snZnNzjB/mJktMrOXzWyJmQ2Jzfumma00s9Vm9j0zs3zuQNHU18N994Xb112X981na6YREWmtFoPezMqAO4ELgNHAVWY2Om2x24F73X0cMBe4LVp3KjANGAe8H/gAcHbeSl9MTz0F27bByJFw+ul537yaaUQkX3Kp0U8B1rn7enc/CMwH0hukRwNPR7cXx+Y7UAH0AHoC5cC29ha6U0idhL3uOmjnlxQ104hIIeUS9IOBTbH71dG0uBXAZdHtS4E+Ztbf3X9HCP43o+FJd1+d/gBmdr2ZVZlZVU1NTWv3oeO9/TY8+mgI+Guuaffm1EwjIoWUr5OxNwJnm9lyQtPMZuCQmb0PGAUMIXw4nGtmZ6av7O7z3H2yu0+urKzMU5EKaP58OHgwVLeHDm335tRMIyKF1D2HZTYD8TQbEk07zN23ENXozexo4HJ332lmnwV+7+57onm/As4AnstD2YsndbVNnk7CzpihJhoRKZxcavRLgZFmNsLMegBXAo/FFzCzAWaW2tZNwF3R7TcINf3uZlZOqO0f0XRTUtasgT/8Afr0gUsvbfXq2X7dKiJSKC0GvbvXAzcATxJC+mF3X2lmc83s4mix6cBaM3sVGAjcGk1fALwG/InQjr/C3R/P7y50sAcfDOOPfxx69Wr16mqPF5GOlkvTDe7+BPBE2rQ5sdsLCKGevt4hoDg9fRXKb34Txm2ozUNoh58zR+3xItJxcgp6ibzzDixdCt27w1lntWkTao8XkY6mLhBa45ln4NAhmDIltNGLiJQABX1rpM6gqkouIiVEQd8aCnoRKUEK+lxt2xa6JT7qqJz7ttGllCLSGSjoc/V01JXPmWdCz545raJLKUWkM1DQ56oNzTbq2kBEOgNdXpmrNgS9LqUUkc5ANfpcrF8PGzdCv34wYUKxSyMi0ioK+lykavPnnANlZcUti4hIKynoc6HLKkWkhCnoW9LQ0HjFjYJeREqQgr4lr7wCNTUweDCcfHKxSyMi0moK+pbEm23a+d+wIiLFoKBvidrnRaTEKeibU1cXeqwEBb2IlCwFfXOWLoU9e+CUU0IbfTPUr42IdFYK+ua0otlG/dqISGeloG/O88+H8TnntLio+rURkc5Kfd00Z+XKMM6h2wP1ayMinZVq9Nns2gXV1VBRASNGFLs0IiJtpqDPZvXqMD71VPVvIyIlTUGfzapVYTx6dHHLISLSTgr6bFLt8wp6ESlxCvpsUjX6MWOKWw4RkXZS0GejphsRSQgFfSa7d8Mbb0CPHvDe9xa7NCIi7ZJT0JvZTDNba2brzGx2hvnDzGyRmb1sZkvMbEg0/Rwzeyk2HDCzj+Z5H/IvfsVNd/3UQERKW4tBb2ZlwJ3ABcBo4CozS2/PuB24193HAXOB2wDcfbG7T3D3CcC5wD7gqfwVv0DUbCMiCZJLjX4KsM7d17v7QWA+cEnaMqOB6G+YWJxhPsDHgF+5+762FrbDKOhFJEFyCfrBwKbY/epoWtwK4LLo9qVAHzPrn7bMlcBDmR7AzK43syozq6qpqcmhSAWmSytFJEHydTL2RuBsM1sOnA1sBg6lZprZIGAs8GSmld19nrtPdvfJlZWVeSpSO+jSShFJkFzONG4GhsbuD4mmHebuW4hq9GZ2NHC5u++MLfJx4BF3r2tXaTvC3r2wcSOUl8NJJxW7NCIi7ZZLjX4pMNLMRphZD0ITzGPxBcxsgJmltnUTcFfaNq4iS7NNp7NmTRiffHIIexGREtdi0Lt7PXADodllNfCwu680s7lmdnG02HRgrZm9CgwEbk2tb2bDCd8Inslv0Qsk1T6vZhsRSYicLhJ39yeAJ9KmzYndXgAsyLLuRo48edt56YobEUkY/TI2nYJeRBJGQZ9Ol1aKSMIo6OP27YMNG0K3ByNHFrs0IiJ5oaCPW7sW3EPI9+hR7NKIiOSFgj5OzTYikkAK+jj9IlZEEkhBH5fDFTeLFsG0aWEsIlIKFPRxOQT9nDnwwgthLCJSChT0KQcOwGuvQVlZ6P4gi7lzYerUMBYRKQX6+6SUtWuhoQFOOQV69sy62IwZYRARKRWq0afoF7EiklAK+hRdWikiCaWgT9GllSKSUAr6lNWrw3jUqOKWQ0QkzxT0KdXVYTx8eFGLISKSbwp6gN27Yc8eOOooOPbYYpdGRCSvFPQAb74ZxieeCGbFLYuISJ4p6AG2bAnjE08sbjlERApAQQ8KehFJNAU9KOhFJNEU9KCgF5FEU9CDgl5EEk1BD41BP2hQccshIlIACnpQjV5EEk1B766gF5FEU9Dv2gX798PRR0OfPsUujYhI3uUU9GY208zWmtk6M5udYf4wM1tkZi+b2RIzGxKb9x4ze8rMVpvZKjMbnsfyt59q8yKScC0GvZmVAXcCFwCjgavMLL3T9tuBe919HDAXuC02717g3919FDAF2J6PgueNgl5EEi6XGv0UYJ27r3f3g8B84JK0ZUYDT0e3F6fmRx8I3d39NwDuvsfd9+Wl5PmioBeRhMsl6AcDm2L3q6NpcSuAy6LblwJ9zKw/cDKw08x+ZmbLzezfo28ITZjZ9WZWZWZVNTU1rd+L9lDQi0jC5etk7I3A2Wa2HDgb2AwcIvz5+JnR/A8A7wWuS1/Z3ee5+2R3n1xZWZmnIuVIQS8iCZdL0G8GhsbuD4mmHebuW9z9MnefCHw9mraTUPt/KWr2qQd+DpyWh3Lnj4JeRBIul6BfCow0sxFm1gO4EngsvoCZDTCz1LZuAu6KrdvXzFLV9HOBVe0vdh4p6EUk4VoM+qgmfgPwJLAaeNjdV5rZXDO7OFpsOrDWzF4FBgK3RuseIjTbLDKzPwEG/Cjve9EeCnoRSThz92KXoYnJkyd7VVVVxzyYO/TsCXV1sHcv9OrVMY8rIpJnZrbM3Sdnmte1fxlbWxtCvm9fhbyIJFbXDno124hIF6CgBwW9iCSagh4U9CKSaAp6UNCLSKIp6EFBLyKJpqAHBb2IJJqCHhT0IpJoCnpQ0ItIonXdoG9ogK1bw+0TTihuWURECqjrBn1NDRw6BAMGhG4QMli0CKZNC2MRkVLVdYM+h2abOXPghRfCWESkVCnomwn6uXNh6tQwFhEpVd2LXYCiySHoZ8wIg4hIKVONXlfciEjCKegV9CKScAp6Bb2IJJyCXkEvIgmnoB80qLjlEBEpsK4Z9PX1sG0bmMHAgcUujYhIQXXNoN+2Lfwx+PHHQ3l5sUsjIlJQXTPo1T4vIl2Igl5EJOEU9CIiCaegFxFJOAW9iEjCKehFRBIup6A3s5lmttbM1pnZ7Azzh5nZIjN72cyWmNmQ2LxDZvZSNDyWz8K3mYJeRLqQFrspNrMy4E7gPKAaWGpmj7n7qthitwP3uvs9ZnYucBtwTTRvv7tPyG+x20lBLyJdSC41+inAOndf7+4HgfnAJWnLjAaejm4vzjC/89izB3bsgO7dobKy2KURESm4XIJ+MLApdr86mha3Argsun0p0MfM+kf3K8ysysx+b2YfzfQAZnZ9tExVTU1N7qVvixdfDONx46CsrLCPJSLSCeTrZOyNwNlmthw4G9gMHIrmDXP3ycBfAd81s5PSV3b3ee4+2d0nVxa6lv3HP4bxBz5Q2McREekkcvkrwc3A0Nj9IdG0w9x9C1GN3syOBi53953RvM3ReL2ZLQEmAq+1t+Btlgr6KVOKVgQRkY6US41+KTDSzEaYWQ/gSqDJ1TNmNsDMUtu6Cbgrmt7PzHqmlgGmAfGTuB1v6dIwVtCLSBfRYtC7ez1wA/AksBp42N1XmtlcM7s4Wmw6sNbMXgUGArdG00cBVWa2gnCS9l/TrtbpWNu3w8aN0Ls3jBpVtGKIiHSkXJpucPcngCfSps2J3V4ALMiw3gvA2HaWMX9StfnJk3UiVkS6jK71y1idiBWRLqhrBb3a50WkC+o6Qe+uK25EpEvqOkG/YQPU1oa/D3zPe4pdGhGRDtN1gj7ePm9W3LKIiHSgrhf0arYRkS6m6wS9TsSKSBfVNYK+vh6WLQu3s1xauWgRTJsWxiIiSdI1gn7lSti/H046Cfr3z7jInDnwwgthLCKSJF0j6HP4odTcuTB1ahiLiCRJTl0glLwc2udnzAiDiEjSdK0avU7EikgXlPyg37sXXnkldGI2cWKxSyMi0uGSH/TLl8OhQzB2LPTqVezSiIh0uOQHfap9Xj1WikgXlfygV/u8iHRxCnoRkYRLzuWVjz0Gn/3skdO3bw9t86NHd3yZREQ6geQE/bvvhlDP5NJLoXtydlVEpDWSk34XXQRbtzbeT3VFbAYDBhSnTCIinUBy2ugrKmDgwMbh+OPh+ONZ9HIl0z5kTTorUwdmItKVJCfos8jUWZk6MBORriTxQZ+pszJ1YCYiXYm5e7HL0MTkyZO9qqqq2MUQESkpZrbM3Sdnmpf4Gr2ISFenoBcRSbicLq80s5nAHUAZ8F/u/q9p84cBdwGVwFvAJ9y9Ojb/GGAV8HN3vyFPZReRElZXV0d1dTUHDhwodlFKSkVFBUOGDKG8vDzndVoMejMrA+4EzgOqgaVm9pi7r4otdjtwr7vfY2bnArcB18Tm/xPwbM6lEpHEq66upk+fPgwfPhxL/e5FmuXu1NbWUl1dzYgRI3JeL5emmynAOndf7+4HgfnAJWnLjAaejm4vjs83s0nAQOCpnEslIol34MAB+vfvr5BvBTOjf//+rf4WlEvQDwY2xe5XR9PiVgCXRbcvBfqYWX8z6wZ8C7ixuQcws+vNrMrMqmpqanIruYiUPIV867XlOcvXydgbgbPNbDlwNrAZOAT8LfBEvL0+E3ef5+6T3X1yZWVlnookIiKQW9BvBobG7g+Jph3m7lvc/TJ3nwh8PZq2EzgDuMHMNhLa8T9pZk1O5IqIFMPOnTv5/ve/36Z1L7zwQnbu3JnfAhVQLkG/FBhpZiPMrAdwJfBYfAEzGxA10wDcRLgCB3e/2t3f4+7DCbX+e919dt5KLyLSRs0FfX19fbPrPvHEE/Tt27cApSqMFoPe3euBG4AngdXAw+6+0szmmtnF0WLTgbVm9irhxOutBSqviHRh+eyQcPbs2bz22mtMmDCBr3zlKyxZsoQzzzyTiy++mNHR/1d89KMfZdKkSYwZM4Z58+YdXnf48OHs2LGDjRs3MmrUKD772c8yZswYzj//fPbv33/EYz3++ON88IMfZOLEiXz4wx9m27ZtAOzZs4dPfepTjB07lnHjxvHTn/4UgF//+tecdtppjB8/nhkzZrR/Z929Uw2TJk1yEUm+VatWtXqdqVPdIYzba8OGDT5mzJjD9xcvXuy9evXy9evXH55WW1vr7u779u3zMWPG+I4dO9zdfdiwYV5TU+MbNmzwsrIyX758ubu7z5o1y++7774jHuutt97yhoYGd3f/0Y9+5F/60pfc3f2rX/2qf/GLX2yy3Pbt233IkCGHy5EqQ1ym5w6o8iy5mpz+6EUk8ebODb3OFqpDwilTpjS5Pv173/sejzzyCACbNm3iz3/+M/3792+yzogRI5gwYQIAkyZNYuPGjUdst7q6miuuuII333yTgwcPHn6MhQsXMn/+/MPL9evXj8cff5yzzjrr8DLHHXdcu/dLXSCISMmYMQOefz6MC6F3796Hby9ZsoSFCxfyu9/9jhUrVjBx4sSM16/37Nnz8O2ysrKM7ftf+MIXuOGGG/jTn/7ED3/4ww7/NbCCXkS6pD59+rB79+6s83ft2kW/fv3o1asXa9as4fe//32bH2vXrl0MHhx+fnTPPfccnn7eeedx5513Hr7/9ttvc/rpp/Pss8+yYcMGAN566602P26Kgl5EuqT+/fszbdo03v/+9/OVr3zliPkzZ86kvr6eUaNGMXv2bE4//fQ2P9Ytt9zCrFmzmDRpEgNif21688038/bbb/P+97+f8ePHs3jxYiorK5k3bx6XXXYZ48eP54orrmjz46aoP3oRKYrVq1czatSoYhejJGV67tQfvYhIF6agFxFJOAW9iEjCKehFRBJOQS8iknAKehGRhFPQi4jk6Oijjy52EdpEQS8iknAKehEpPrPCDM2YPXt2k+4HbrnlFm6//Xb27NnDjBkzOO200xg7diyPPvpoi8XP1p1xpu6Gs3VNXFDZurUs1qBuikW6hiZd7UJhhma8+OKLftZZZx2+P2rUKH/jjTe8rq7Od+3a5e7uNTU1ftJJJx3uYrh3794Zt5WpO+Ns3Q1n6pq4tdRNsYiUniJ0xTJx4kS2b9/Oli1bqKmpoV+/fgwdOpS6ujq+9rWv8eyzz9KtWzc2b97Mtm3bOOGEE7JuK1N3xjU1NRm7G87UNXGhKehFpMuaNWsWCxYsYOvWrYc7D3vggQeoqalh2bJllJeXM3z48Ga7FY53Z9yrVy+mT5/e4d0Qt0Rt9CLSZV1xxRXMnz+fBQsWMGvWLCB0KXz88cdTXl7O4sWLef3115vdRrbujLN1N5ypa+JCS1TQ5/P/JEUk+caMGcPu3bsZPHgwgwYNAuDqq6+mqqqKsWPHcu+993Lqqac2u41s3Rln6244U9fEhZaoboqnTYMXXoCpU8O/0IhI56VuituuS3dTPHduCPlC/Z+kiEgpStTJ2BkzCvdfkiIipSpRNXoRKS2drem4FLTlOVPQi0hRVFRUUFtbq7BvBXentraWioqKVq2XqKYbESkdQ4YMobq6mpqammIXpaRUVFQwZMiQVq2joBeRoigvLz/8q1EpLDXdiIgknIJeRCThFPQiIgnX6X4Za2Y1QPOdSzRvALAjT8UptiTtCyRrf5K0L6D96cxy3Zdh7l6ZaUanC/r2MrOqbD8DLjVJ2hdI1v4kaV9A+9OZ5WNf1HQjIpJwCnoRkYRLYtDPa3mRkpGkfYFk7U+S9gW0P51Zu/clcW30IiLSVBJr9CIiEqOgFxFJuMQEvZnNNLO1ZrbOzGYXuzytZWZ3mdl2M3slNu04M/uNmf05Ghf+7+LzwMyGmtliM1tlZivN7IvR9FLdnwoz+6OZrYj25xvR9BFm9ofomPuJmfUodllzZWZlZrbczH4R3S/lfdloZn8ys5fMrCqaVpLHGoCZ9TWzBWa2xsxWm9kZ7d2fRAS9mZUBdwIXAKOBq8xsdHFL1Wp3AzPTps0GFrn7SGBRdL8U1ANfdvfRwOnA56PXo1T3513gXHcfD0wAZprZ6cC/Ad9x9/cBbwOfKV4RW+2LwOrY/VLeF4Bz3H1C7HrzUj3WAO4Afu3upwLjCa9T+/bH3Ut+AM4Anozdvwm4qdjlasN+DAdeid1fCwyKbg8C1ha7jG3cr0eB85KwP0Av4EXgg4RfK3aPpjc5BjvzAAyJwuJc4BeAleq+ROXdCAxIm1aSxxpwLLCB6EKZfO1PImr0wGBgU+x+dTSt1A109zej21uBgcUsTFuY2XBgIvAHSnh/oqaOl4DtwG+A14Cd7l4fLVJKx9x3ga8CDdH9/pTuvgA48JSZLTOz66NppXqsjQBqgP+Omtb+y8x60879SUrQJ56Hj/KSuhbWzI4Gfgr8nbu/E59Xavvj7ofcfQKhNjwFOLW4JWobM/tLYLu7Lyt2WfLoQ+5+GqHp9vNmdlZ8Zokda92B04AfuPtEYC9pzTRt2Z+kBP1mYGjs/pBoWqnbZmaDAKLx9iKXJ2dmVk4I+Qfc/WfR5JLdnxR33wksJjRv9DWz1J/3lMoxNw242Mw2AvMJzTd3UJr7AoC7b47G24FHCB/EpXqsVQPV7v6H6P4CQvC3a3+SEvRLgZHRlQM9gCuBx4pcpnx4DLg2un0toa270zMzA34MrHb3b8dmler+VJpZ3+j2UYTzDasJgf+xaLGS2B93v8ndh7j7cML75Gl3v5oS3BcAM+ttZn1St4HzgVco0WPN3bcCm8zslGjSDGAV7d2fYp98yONJjAuBVwltp18vdnnaUP6HgDeBOsKn+mcIbaeLgD8DC4Hjil3OHPflQ4Svli8DL0XDhSW8P+OA5dH+vALMiaa/F/gjsA74H6Bnscvayv2aDvyilPclKveKaFiZeu+X6rEWlX0CUBUdbz8H+rV3f9QFgohIwiWl6UZERLJQ0IuIJJyCXkQk4RT0IiIJp6AXEUk4Bb2ISMIp6EVEEu5/AUg/D0Kd1yMtAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#v2: test AE mdoel metrics\n",
    "\n",
    "\n",
    "# plot loss curve on validation data\n",
    "import pandas as pd\n",
    "import xlrd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "for i in range(run_times):\n",
    "    print('#**************************************************************************#')\n",
    "    print('plot i_th fig:', (i+1))\n",
    "    \n",
    "    # load loss from excels\n",
    "\n",
    "    file_name = 'ae_hla_loss_acc_v'+str(i)+'_missing_ratio_'+str(missing_perc)+'.xlsx'\n",
    "    data_file = os.path.join(data_path, file_name)\n",
    "\n",
    "    \n",
    "    \n",
    "    df = pd.read_excel(data_file)\n",
    "\n",
    "\n",
    "    # plot loss\n",
    "    \n",
    "    loss1 = df.iloc[0,1:].values.tolist()\n",
    "    val_loss1 = df.iloc[1,1:].values.tolist()\n",
    "    \n",
    "\n",
    "\n",
    "    print('loss1 start:', loss1[0:5])\n",
    "    print('loss1 end:', loss1[95:100])\n",
    "    \n",
    "    print('val_loss1 start:', val_loss1[0:5])\n",
    "    print('val_loss1 end:', val_loss1[95:100])\n",
    "\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(range(len(loss1)), loss1, 'bo', linewidth=2, markersize=1.5, label='train loss')\n",
    "    plt.plot(range(len(val_loss1)), val_loss1, 'r', linewidth=2, markersize=1.5, label='val loss')\n",
    "\n",
    "    #plt.title('AE U19 loss with missing ratio of '+str(missing_perc)+' at time '+str(i+1))\n",
    "    plt.title('AE HLA loss with missing ratio of '+str(missing_perc)+' at time '+str(i+1))\n",
    "\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    # plot accuracy\n",
    "\n",
    "\n",
    "    acc1 = df.iloc[2,1:].values.tolist()\n",
    "    val_acc1 = df.iloc[3,1:].values.tolist()\n",
    "    \n",
    "    print('acc1 start:', acc1[0:5])\n",
    "    print('acc1 end:', acc1[95:100])\n",
    "    \n",
    "    print('val_acc1 start:', val_acc1[0:5])\n",
    "    print('val_acc1 end:', val_acc1[95:100])    \n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(range(len(acc1)), acc1, 'bo', linewidth=2, markersize=1.5, label='train acc')\n",
    "    plt.plot(range(len(val_acc1)), val_acc1, 'r', linewidth=2, markersize=1.5, label='val acc')\n",
    "\n",
    "    #plt.title('AE U19 accuracy with missing ratio of '+str(missing_perc)+' at time '+str(i+1))\n",
    "    plt.title('AE HLA accuracy with missing ratio of '+str(missing_perc)+' at time '+str(i+1))\n",
    "\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#**************************************************************************#\n",
      "ae with time: 1\n",
      "avg_acc: 0.944231\n",
      "avg_r2: 0.70266\n",
      "avg_pcc: 0.807199\n",
      "avg_hen: 0.948322\n",
      "avg_hen_min: 0.515345\n",
      "avg_sen: 0.995078\n",
      "avg_sen_min: 0.830217\n",
      "avg_iqs: 0.902145\n",
      "#**************************************************************************#\n",
      "avt_acc: [0.944231]\n",
      "#**************************************************************************#\n",
      "avt_acc: 0.944231 0.0\n",
      "#**************************************************************************#\n",
      "avt_acc: [0.944231, 0.944231, 0.0]\n",
      "metrics: [[0.944231, 0.944231, 0.0], [0.70266, 0.70266, 0.0], [0.807199, 0.807199, 0.0], [0.948322, 0.948322, 0.0], [0.515345, 0.515345, 0.0], [0.995078, 0.995078, 0.0], [0.830217, 0.830217, 0.0], [0.902145, 0.902145, 0.0]]\n",
      "cols_label: ['time_1', 'mean', 'std']\n",
      "rows_label: ['ae_avg_acc', 'ae_avg_r2', 'ae_avg_pcc', 'ae_avg_hen', 'ae_avg_hen_min', 'ae_avg_sen', 'ae_avg_sen_min', 'ae_avg_iqs']\n"
     ]
    }
   ],
   "source": [
    "# calculate average metric and std\n",
    "\n",
    "import pandas as pd\n",
    "import xlrd\n",
    "import statistics\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "avg_acc = []\n",
    "avg_r2 = []\n",
    "avg_pcc = []\n",
    "avg_hen = []\n",
    "avg_hen_min = []\n",
    "avg_sen = []\n",
    "avg_sen_min = []\n",
    "avg_iqs = []\n",
    "\n",
    "for i in range(run_times):\n",
    "    print('#**************************************************************************#')\n",
    "    print('ae with time:', (i+1))\n",
    "    \n",
    "\n",
    "    file_name = 'ae_hla_metrics_avg_v'+str(i)+'_missing_ratio_'+str(missing_perc)+'.xlsx'\n",
    "    data_file = os.path.join(data_path, file_name)\n",
    "\n",
    "    \n",
    "\n",
    "    #df = pd.read_excel(data_file, sheet_name='Sheet1')\n",
    "    df = pd.read_excel(data_file)\n",
    "\n",
    "    d = df.iloc[0,1:2].values.tolist()[0]\n",
    "    #d = df.iloc[0,1:2].values\n",
    "    print('avg_acc:', d)\n",
    "    avg_acc.append(d)\n",
    "    \n",
    "    d = df.iloc[1,1:2].values.tolist()[0]\n",
    "    print('avg_r2:', d)\n",
    "    avg_r2.append(d)    \n",
    "\n",
    "    d = df.iloc[2,1:2].values.tolist()[0]\n",
    "    print('avg_pcc:', d)\n",
    "    avg_pcc.append(d)  \n",
    "    \n",
    "    d = df.iloc[3,1:2].values.tolist()[0]\n",
    "    print('avg_hen:', d)\n",
    "    avg_hen.append(d)  \n",
    "    \n",
    "    d = df.iloc[4,1:2].values.tolist()[0]\n",
    "    print('avg_hen_min:', d)\n",
    "    avg_hen_min.append(d)  \n",
    "    \n",
    "    d = df.iloc[5,1:2].values.tolist()[0]\n",
    "    print('avg_sen:', d)\n",
    "    avg_sen.append(d)      \n",
    "    \n",
    "    d = df.iloc[6,1:2].values.tolist()[0]\n",
    "    print('avg_sen_min:', d)\n",
    "    avg_sen_min.append(d)     \n",
    "    \n",
    "    d = df.iloc[7,1:2].values.tolist()[0]\n",
    "    print('avg_iqs:', d)\n",
    "    avg_iqs.append(d)      \n",
    "    \n",
    "\n",
    "    \n",
    "# metrics\n",
    "print('#**************************************************************************#')\n",
    "print('avt_acc:', avg_acc)    \n",
    "\n",
    "print('#**************************************************************************#') \n",
    "\n",
    "\n",
    "acc_mean = np.mean(avg_acc)\n",
    "\n",
    "r2_mean = np.mean(avg_r2)\n",
    "pcc_mean = np.mean(avg_pcc)\n",
    "hen_mean = np.mean(avg_hen)\n",
    "hen_min_mean = np.mean(avg_hen_min)\n",
    "sen_mean = np.mean(avg_sen)\n",
    "sen_min_mean = np.mean(avg_sen_min)\n",
    "iqs_mean = np.mean(avg_iqs)\n",
    "\n",
    "\n",
    "acc_std = np.std(avg_acc)\n",
    "r2_std = np.std(avg_r2)\n",
    "pcc_std = np.std(avg_pcc)\n",
    "hen_std = np.std(avg_hen)\n",
    "hen_min_std = np.std(avg_hen_min)\n",
    "sen_std = np.std(avg_sen)\n",
    "sen_min_std = np.std(avg_sen_min)\n",
    "iqs_std = np.std(avg_iqs)\n",
    "\n",
    "\n",
    "print('avt_acc:', acc_mean, acc_std)    \n",
    "\n",
    "    \n",
    "print('#**************************************************************************#') \n",
    "    \n",
    "# write out\n",
    "avg_acc.append(acc_mean)\n",
    "avg_acc.append(acc_std)\n",
    "#avg_acc.append(acc_p)\n",
    "\n",
    "avg_r2.append(r2_mean)\n",
    "avg_r2.append(r2_std)\n",
    "\n",
    "avg_pcc.append(pcc_mean)\n",
    "avg_pcc.append(pcc_std)\n",
    "\n",
    "\n",
    "avg_hen.append(hen_mean)\n",
    "avg_hen.append(hen_std)\n",
    "\n",
    "avg_hen_min.append(hen_min_mean)\n",
    "avg_hen_min.append(hen_min_std)\n",
    "\n",
    "avg_sen.append(sen_mean)\n",
    "avg_sen.append(sen_std)\n",
    "\n",
    "avg_sen_min.append(sen_min_mean)\n",
    "avg_sen_min.append(sen_min_std)\n",
    "\n",
    "avg_iqs.append(iqs_mean)\n",
    "avg_iqs.append(iqs_std)\n",
    "\n",
    "\n",
    "\n",
    "print('avt_acc:', avg_acc)    \n",
    "\n",
    "metrics = [avg_acc, avg_r2, avg_pcc, avg_hen, avg_hen_min, avg_sen, avg_sen_min, avg_iqs]\n",
    "\n",
    "print('metrics:', metrics)        \n",
    " \n",
    "\n",
    "file_name = 'ae_hla_metrics_times_'+str(run_times)+'_missing_ratio_'+str(missing_perc)+'_v1.1.xlsx'\n",
    "file_path_out = os.path.join(data_path, file_name)\n",
    "\n",
    "\n",
    "cols_label = [0 for i in range(run_times+2)]\n",
    "\n",
    "for i in range(run_times):\n",
    "    cols_label[i] = 'time_'+str(i+1)\n",
    "\n",
    "cols_label[run_times] = 'mean' \n",
    "cols_label[run_times+1] = 'std' \n",
    "\n",
    "print('cols_label:', cols_label)    \n",
    "\n",
    "\n",
    "\n",
    "rows_label = ['ae_avg_acc', 'ae_avg_r2', 'ae_avg_pcc', \n",
    "              'ae_avg_hen', 'ae_avg_hen_min', 'ae_avg_sen',\n",
    "             'ae_avg_sen_min', 'ae_avg_iqs']\n",
    "\n",
    "print('rows_label:', rows_label)    \n",
    "\n",
    "df = pd.DataFrame(data = np.array(metrics), index = rows_label, columns = cols_label)\n",
    "df.to_excel(file_path_out, na_rep='NA', header=True, index=True, float_format=\"%.6f\")  \n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
